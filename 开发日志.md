# MyCppServerLib 开发日志

服务器的生命周期如下：


- socket: 创建套接字。
    - 这是一个整数句柄，指向一个内核中的结构体
- bind: 绑定 IP 和端口（这相当于实例化了这个 socket 的存在本身）。
- listen: 监听（这会让 OS 将这个 socket 标记为监听类 socket）。
- accept: 接收连接（这是一个阻塞操作，程序会停在这里直到有客户端连上来）。
- read/write: 通信


socket 分为两种类型：
- 监听socket：用于监听客户端连接的套接字。（永远不会有目的端口，只是为了处理建立新连接的请求）
    - 它具有两个队列：
        - 已完成连接队列：存储已经完成三次握手的连接，等待服务器调用 `accept` 来处理。
        - 半连接队列：存储正在进行三次握手的连接，等待完成后进入已完成连接队列。
        - 握手过程是 OS 的责任，服务器程序不需要关心
- 连接socket：用于与客户端通信的套接字。（含有完整的五元组：源IP、源端口、目的IP、目的端口、协议）
    - 它具有发送和接受缓冲区。


IO复用：一个线程（服务器线程，复用者）同时监控多个
socket_fd（文件描述符，对应于多个IO事件）


Epoll的生命周期如下：
- 创建epoll实例：调用 `epoll_create` 创建一个epoll实例。
- 注册文件描述符：使用 `epoll_ctl` 将监听socket注册到epoll实例中，监听可读事件。
    - 具体而言，需要创建一个`epoll_event`类型的数组，作为事件列表的返回位置
    - 需要创建一个`epoll_event`类型的变量：
        - 设置其成员 `events` 为 `EPOLLIN`（表示监听可读事件）
        - 为什么要设置这个？
        - 因为我们要告诉Epoll，我们对这个 socket 感兴趣的事件类型是什么
        - 一个 socket 在以下几种情况是活跃的：
            - 可读：有数据可读，或者有新的连接请求（对于监听socket）
            - 可写：发送缓冲区有空间可写
            - 错误：发生错误
            - 关闭：连接被关闭
        - 如果这里我们不设置`EPOLLIN`，Epoll将会因为socket可写而持续触发事件，导致CPU占用率飙升
        - 即使你明确了只关注“可读（EPOLLIN）”，内核依然不知道你喜欢哪种通知方式
            - 水平触发 (Level Triggered, LT) —— 默认模式
            - 只要缓冲区里还有哪怕 1 个字节的数据，内核就会在每一次 epoll_wait 时把你叫醒。
            - 边缘触发 (Edge Triggered, ET) —— 需要额外设置 `EPOLLET` 标志
            - 状态发生突变时，只通知你一次。
            - 突变的定义：新的连接建立，新数据到来，（对于写）发送缓冲区从满变不满。
        - 将监听socket的文件描述符赋值给 `data.fd`
        - 为什么？
        - 如我们所知，Epoll要返回的是一个事件表
        - 那这个事件表中每个元素都是什么，都包含什么？
        - 显然是一个 `epoll_event` 结构体
        - Epoll 不能自己凭空生成事件
        - 它返回的就是我们在这个注册函数中传入的事件变量ev
    - 调用 `epoll_ctl`，传入epoll实例的
        - 文件描述符
        - 操作类型（如 `EPOLL_CTL_ADD` 表示添加）
        - 监听socket的文件描述符
        - 事件变量ev
        - `epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);`
- 服务器进入主循环
- 等待事件：调用 `epoll_wait` 等待事件发生。
    - 这个函数在做什么？
    - 这个函数会阻塞，直到有事件发生
    - 更具体而言，如果没有事件发生，程序会停在这里
    - 然后 OS 会将这个线程设置为可运行状态，当有事件发生时，OS 会将这个线程唤醒
    - 当有事件发生时，`epoll_wait` 会将就绪的事件放入我们之前创建的事件列表中
    - 并返回就绪事件的数量
    - 下一步就要我们来处理这些就绪事件了
- 对于每个就绪事件，检查事件类型并处理相应的事件：
    - 首先，一个 socket 可读，意思就是这个 socket 的接收缓冲区中有数据可读
    - 然后，一个 socket 可写，意思就是这个 socket 的发送缓冲区中有空间可写
    - 现在让我们开始分类处理事件
    - 如果事件是监听socket的可读事件：
        - 说明有新的连接请求到来
        - 我们需要调用 `accept` 来接受这个连接
        - 并将新的连接socket注册到epoll实例中，监听可读事件。
    - 如果事件是连接socket的可读事件：
        - 说明这个连接有数据可读
        - 我们需要调用 `read` 来读取数据
        - 并根据需要进行处理。
    - 如果事件是连接socket的可写事件：
        - 说明这个连接的发送缓冲区有空间可写
        - 我们需要调用 `write` 来发送数据。
    - 如果事件是连接socket的错误事件：
        - 说明这个连接发生了错误
        - 我们需要关闭这个连接，并从epoll实例中删除它。
- 循环继续等待事件，直到服务器关闭。


Epoll的目的：
- 高效的事件通知机制：Epoll使用事件驱动模型，避免了轮询带来的性能问题。
- 支持大量连接：Epoll能够同时处理成千上万的连接，不阻塞。
- 较低的系统调用开销：Epoll通过内核事件表来管理文件描述符，减少了系统调用的次数。

Epoll的工作原理：
- Epoll使用一个内核事件表来管理文件描述符。当一个文件描述符注册到epoll实例中时，内核会将其添加到事件表中。
- 这个表是基于红黑树和链表实现的。：
  - 红黑树用于存储所有注册的文件描述符，支持高效的插入、删除和查找操作。
  - 链表用于存储就绪的文件描述符，支持快速遍历和处理。
- 应用程序通过调用 `epoll_wait` 来等待事件的发生。当有事件发生时，应用程序会收到一个事件列表，其中包含了发生事件的文件描述符和事件类型。
- 应用程序可以根据事件类型来处理相应的事件，例如读取数据、发送数据或关闭连接。


- 为什么我们使用 epoll 的流程如此繁琐？
    - epoll 是一个内核中的机制
    - 如果允许用户态程序直接操作 epoll 的数据结构，可能会导致内核崩溃
    - 同时用户态程序也不应该负责处理 epoll 需要解决的同步问题（如线程安全、事件分发等）
    - 这也是将操作系统分离为内核态和用户态的原因之一
- 这里为了保持代码的清晰和可维护，我们需要封装 epoll 的相关操作
    - 这也是我们引入 Epoll 类的原因
    - 流程化的 epoll 操作代码被我们封装在 Epoll 类的成员函数中
    - 这样我们就可以通过调用这些成员函数来完成 epoll 的相关操作
    - 而不需要直接操作 epoll 的数据结构
    - 也不会做“在 main 函数里放置一个全局变量（事件数组）来接收 epoll_wait 的返回值“这种事情


我会发现有一大堆类型，都看起来像是无符号32位整数：
- `uint32_t`：无符号32位整数，范围从0到4294967295。
- `unsigned int`：无符号整数，通常也是32位，但具体大小取决于平台。
- 为什么会有一大堆？
- 根本原因：历史 + 平台差异 + 标准层叠
- 最原始的C标准只保证：
    - sizeof(short) <= sizeof(int) <= sizeof(long)
    - 但没有规定：
        - int 是不是 32 位？
        - long 是不是 64 位？
        - 在windows上，int 是 32 位，long 也是 32 位
        - 在linux上，int 是 32 位，long 是 64 位
        - 这就导致了平台差异
- 为了应对这种差异，C99引入了 `<stdint.h>` 头文件
- 定义了固定宽度的整数类型，如 `uint32_t` 和 `int64_t`
- 本质上大部分是`typedef unsigned int uint32_t;`这样的定义



目前，我们使用Epoll的方式非常原始，这表现为：
    - 我们只使用 fd 一个整数来代表一个连接
    - 仅仅通过 fd 无法描述完整的连接状态
        - 比如：这个 fd 是监听socket 还是连接socket？
        - 我们应该对它进行 accept 还是 read/write？
    - 这些信息的确定发生在我们将 fd 注册到 Epoll 实例中的时候
        - 例如，我们调用 Epoll 的方法设置EPOLLIN
        - 调用 Epoll 的方法将 fd 注册到 Epoll 实例中
    - 在现在的实现中，这些过程都发生在 main 函数里
    - 换句话说，这些属于 fd 的信息被隐含在了 main 函数里
    - 这不符合面向对象的设计原则，会导致代码的不可维护
    - 因此我们需要引入一个新的类来封装这些信息
解决方案：
- 我们设计一个 Channel 类，把下述信息打包在一起：
    - fd：文件描述符。
    - events：我们要监听这 fd 的什么事件（如 EPOLLIN）。
    - revents：目前实际在这个 fd 上发生了什么事件（received events）。
    - Epoll*：这个 Channel 属于哪个 Epoll 实例管（方便直接通过 Channel 调用 update）。
- 简而言之：Channel = fd + 其相关的事件 + 其属性。
- 这样一来，我们作注册的时候，在 main 函数里表现为：
    - 创建一个 Channel 实例，传入 fd 和事件信息
    - 调用 Channel 的方法将自己注册到 Epoll 实例中
- 同时，我们对 Epoll 的任何操作也都必须通过 Channel 来完成
    - 例如：更新事件信息，删除事件等
    - 我们仅在初始化时直接操作了 Epoll 来创建实例


现在 server.cpp 里还是充满了 while(true) 循环，以及一堆判断 events 的逻辑。
- 这依然是面向过程的写法。
- 我们要把 main 函数里那个 while(true) 循环，以及里面的 epoll_wait 逻辑，全部移到一个叫 EventLoop 的类里面去。
- 这样我们就可以在创建监听socket以后，用一行代码完成：
    - 把监听socket注册到 EventLoop 里
    - 定义好这个监听socket的事件处理逻辑
    - 比如：当有新连接时，调用 accept，然后把新的连接socket注册到 EventLoop 里
    - 这都是服务器socket需要做的配置
- 我们解决这一问题的方式就是：引入事件循环
- 也就是把 main 函数里那个 while(true) 循环，以及里面的 epoll_wait 逻辑，全部移到一个叫 EventLoop 的类里面去


现在我们的服务器逻辑没有变化，但其代码结构变化如下：
- 我们已经让 Channel 类持有了 fd 和事件相关的信息
    - 但是这还不够
    - 我们还需要让 Channel 类持有事件的处理逻辑
    - 也就是说，我们需要让 Channel 类持有一个回调函数（callback）
    - 这个回调函数会在事件发生时被调用
    - 实现方式就是增加一个成员变量：std::function<void()> callback;
- 然后我们将引入事件循环 EventLoop 类
    - 原本的 Channel 类持有了一个 Epoll* 指针，表示这个 Channel 属于哪个 Epoll 实例管
    - 我们原本这样做的意图如前文所述，是为了将无法用 fd 来描述的连接信息封装在 Channel 类里
    - 但是显然，Epoll 是全局性的资源，所有 Channel 都属于同一个 Epoll 实例管
    - 于是我们将 Channel 类中的 Epoll* 指针替换为 EventLoop* 指针
    - 转而让 EventLoop 类来持有 Epoll 实例
    - 同时将循环处理一次“epoll_wait返回的事件列表”的逻辑放到 EventLoop 类里
    - 具体而言，EventLoop 类会有一个成员函数 loop()
    - 在这个函数里会有一个 while(true) 循环
    - 在这个循环里会调用 Epoll 的 wait() 方法来等待事件的发生
    - 当事件发生时，这个函数还会负责遍历事件列表
    - 然后调用每个事件对应的 Channel 的回调函数来处理事件


现在我们成功的将服务器的事件处理逻辑封装在了 Channel 类里，并将事件循环的逻辑封装在了 EventLoop 类里。

但是还有一个问题，我们在创建服务器时，仍然需要：
- 在 main 函数里写一大段代码来创建监听 channel 
    - 这是 Channel 的初始化
    - 是建立一个服务器的必要步骤其一
- 在 main 函数原本的那个 while(true) 循环里写一个判断：
    - 如果事件是监听 channel 的可读事件：
        - 说明有新的连接请求到来
        - 我们需要调用 accept 来接受这个连接
        - 并将新的连接 channel 注册到epoll实例中，监听可读事件。
    - 如果事件是连接 channel 的可读事件：
        - 说明这个连接有数据可读
        - 我们需要调用 read 来读取数据
        - 并根据需要进行处理。
    - 现在看看我们的情况：
        - 我们将连接 channel 的可读事件的处理逻辑封装在了 Channel 类里，作为回调函数存在
        - 这个回调函数的调用又被 EventLoop 的事件循环封装了
        - 换句话说，整个循环中的第二个分支判断已经被我们封装了
        - 显然，我们接下来要做的就是把监听 channel 处理与它相关的事件的逻辑，也接入到 EventLoop 里
- 梳理一下，我们要做的服务器建立需要两步：
    - 创建监听 channel
    - 定义好这个监听 channel 的事件处理逻辑
        - 也就是前面反复说的，当有新连接时，调用 accept，然后把新的连接 channel 注册到 EventLoop 里
        - 当有数据可读时，调用 read 来读取数据，并根据需要进行处理。
- 只要我们建立一个`Server`类来完成这两步，我们就可以在 main 函数里用一行代码来创建服务器了
- 这就是我们引入 Server 类的原因
    - 其有一个成员函数`handleReadEvent()`，描述了监听 channel 的事件处理逻辑
        - 我们通过`bind`函数将它的签名适配为`std::function<void()>`
        - 并将其作为监听channel的回调函数，传入构造函数中
        - 回顾我们已经做的层层封装：
            - 当 Channel 的构造函数被调用
            - 它会通过我们传入的 EventLoop* 指针来建立与 EventLoop 的联系
            - 此时它还只是一个 Channel 实例，尚未注册到 epoll 里
            - 我们随后调用了 Channel 的 enableReading() 方法
            - 这个方法会调用 EventLoop 的 updateChannel() 方法
            - 这个方法又会调用 Epoll 的 updateChannel() 方法
            - 此时 Channel 才真正被注册到 epoll 里
            - 同理，以后 Eventloop 也会在每次循环时通过其持有的 Epoll 实例来调用 Epoll 的 poll() 方法
            - 这个方法会调用 OS 提供的 Epoll 的 epoll_wait() 方法来等待事件的发生
            - 事件发生后，也会逆向顺着以上的调用链引发 Channel 的回调函数调用
    - 其有一个成员变量`EventLoop* loop_`，表示这个服务器属于哪个事件循环
    - 我们将这个 EventLoop* 指针保存在 Server 类中，以便后续可以调用其 loop() 方法来启动事件循环

现在我们可以在 main 函数里用一行代码来创建服务器了：
- 让我们再梳理一遍从的服务器创建开始，到有第一个客户端连上来之间的完整过程：
    - 首先，我们在 main 函数里创建一个 EventLoop 实例，表示我们的事件循环。
    - 然后，我们创建一个 Server 实例，传入 EventLoop 的指针。
        - 在 Server 的构造函数里，我们会创建一个监听 channel，并将其注册到 EventLoop 里
        - 也就是说，他会被注册到 EventLoop 持有的 Epoll 实例里
        - 同时我们还会定义好这个监听 channel 的事件处理逻辑
        -（当有新连接时，调用 accept，然后把新的连接 channel 注册到 EventLoop 里）
        - 这个时候程序还是在简单顺序执行的，没有阻塞在任何地方
    - 接下来，我们调用 EventLoop 的 loop() 方法来启动事件循环。
        - 这个方法会进入一个 while(true) 循环
        - 并在循环里调用 Epoll 的 poll() 方法
        - 程序在此发生阻塞，等待 epoll 的回应导致 OS 将这个线程唤醒
    - 此时，服务器已经开始监听客户端的连接请求了。
    - 当有第一个客户端连上来时，监听 channel 的可读事件就会被触发。
    - 这个事件会被 EventLoop 通过上述调用链捕获
    - 并调用对应的 Channel 的回调函数来处理这个事件。
    - 在这个回调函数里，我们会调用 accept 来接受这个连接，配置它的业务函数，将其作为新的连接 channel 注册到 EventLoop 里。
    - 随后，程序继续留在 EventLoop 的事件循环里，等待下一个事件的发生。
    - 同样的，程序阻塞在 epoll_wait 里，被挂起，等待 OS 的唤醒。

为什么要把 Server 和 EventLoop 分开？
- 这也是面向对象设计的原则之一：单一职责原则
    - Server 的职责是：
        - 监听端口
        - accept 新连接
        - 创建连接对象
        - 设置回调
        - 维护连接集合
        - 关闭连接
    - EventLoop 的职责是：
        - epoll_wait
        - 维护 fd 列表
        - 触发回调
        - 它不应该知道：
            - 业务协议
            - 连接生命周期策略
            - 线程池
        - 它是纯粹的 IO 调度器。
- 分开的真正主要原因：
    - 这样我们就可以在不同的服务器实现中复用同一个 EventLoop 类了
    - 例如，我们可以实现一个 HTTP 服务器，一个 FTP 服务器，甚至一个 WebSocket 服务器
    - 这些服务器的业务逻辑完全不同，但它们都可以使用同一个 EventLoop 来处理 IO 事件
    - 我们可以：
        - 一个 Server 用一个 EventLoop
        - 多个 Server 共用一个 EventLoop
        - 一个线程一个 EventLoop（多 Reactor 模式，这个后面再解释）

我们分离了服务器类和事件驱动类，将服务器逐渐开发成Reactor模式。
至此，所有服务器逻辑（目前只有接受新连接和echo客户端发来的数据）都写在`Server`类里。
但很显然，`Server`作为一个服务器类，应该更抽象、更通用。
Day 6 的代码虽然能跑，但 Server 类承担了太多的责任：
- 既要负责 "监听连接"（Bind/Listen/Accept）
- 又要负责 "处理新连接的业务"（Read/Write）。
我们要把 "只负责把新连接接进来的组件" 剥离出来，这就是 Acceptor。
- Acceptor：专注于 Listen Socket。
    - 它持有监听套接字，关注连接事件。
    - 当有连接进来时，它 accept 得到 fd，然后把它扔给 Server。
- Server：专注于业务调度。
    - 它不再管 bind/listen 这些底层细节
    - 只等待 Acceptor 给它塞一个新的连接 fd，然后处理后续逻辑。

代码现在逻辑已经非常清晰了：Acceptor 管进门，Server 管调度。

但是，我们一直搁置了一个巨大的隐患：生命周期管理。
到现在为止，我们还在 Server 的回调里写着：
    - new Socket
    - new Channel
    - 在用临时的 lambda 表达式处理读写
- 一旦连接断开，谁来负责删除 Socket, Channel, InetAddress？
- 现在的代码里充满了 delete 的坑，或者干脆就是内存泄漏。


引入 Connection 类。
- 这是 TCP 连接的终极封装。
    - 它持有：
        - Socket
        - Channel
        - EventLoop。
    - 它负责：
        - 处理这条连接的所有读写事件（不再用临时 lambda）。
    - 它管理：
        - 资源的销毁。
        - 当连接断开时，Connection 析构，顺带把 socket 和 channel 带走。
Server 将不再直接持有 socket，而是持有 std::map<int, Connection*>。


修改好后在 Day 8 的代码中，我们手动管理 new 和 delete：
- Acceptor new 出来的 Socket 交给 Connection 持有。
- Connection new 出来交给 Server 的 map 持有。
- 客户端断开导致以下调用链：
    - 触发 Connection::echoRead
    - 调用回调 Server::deleteConnection
    - 从 map 移除并 delete Connection
    - Connection 析构
    - delete Socket。
这一条链非常清晰，是 C++ 项目中经典的资源管理模式。
在更现代的 C++ 中，这里通常会用 std::shared_ptr 和 std::enable_shared_from_this 来自动管理



到目前为止，数据读写是非常“暴力”的：
```
char buf[1024];
read(fd, buf, ...);
write(fd, buf, ...);
```
这有两个严重问题：
- 应用层粘包/半包（Packet Sticking/Splitting）：
    - TCP 是字节流协议，没有“包”的概念。
    - 你发了 "Hello"，又发了 "World"
    - 服务器可能会收到 "HelloWor"，也可能收到 "He" 和 "lloWorld"
    - 协议栈不承诺你每次 read 就能读到一个完整的逻辑包
    - 它只保证顺序传输和数据完整性
    - 单纯的 read(buf) 无法处理这种情况
    - 我们需要一个缓冲区先把数据攒着，凑够一个完整的逻辑包再处理。
- 写阻塞：
    - 如果发送大量数据，write 缓冲区满了
    - 也就是协议栈内部的发送缓冲区满了（这个过程我们无法控制）
    - 调用 write 会阻塞（或者返回 EAGAIN）。
    - 在 Reactor 模式下，绝对不能阻塞。
    - 因为如果阻塞了，整个事件循环就停了，其他连接的事件也无法处理了。
    - 如果一次写不完，我们需要把剩下的数据存起来，等 socket 可写了（EPOLLOUT）再接着写。
比 Day 8，Day 9 引入 Buffer 类来解决这两个问题：
- Day 8 (无 Buffer)
    - 读数据	直接 read 到栈上 char buf[1024]。
    - 数据边界	读到多少算多少，可能只读到 "Hel"。没地方存，必须马上处理。
    - 写数据	直接 write。如果内核缓冲区满了，剩下的数据直接丢弃（或阻塞线程）。
    - 写完成	发送完就完了，不知道什么时候还可以继续写。
- Day 9 (有 Buffer)
    - 先 readv 到栈，然后 append 进 inputBuffer_。
    - 数据都攒在 inputBuffer_ 里，业务逻辑可以只有在攒够一个完整包（比如等到 \n）时才处理。
    - 先试着 write。如果没写完，剩下的存进 outputBuffer_，并自动关注 EPOLLOUT 事件。
    - 当 socket 变为空闲（触发 EPOLLOUT），自动继续发送 Buffer 里剩余的数据。

数据流追踪：从“接收”到“发送”
假设客户端发来 "Hello World"。

1. 读流程 (Incoming)
    - 触发点：内核通知 Epoll：“Hey，fd 5 有数据来了 (EPOLLIN)”。
    - EventLoop::loop() 被 epoll_wait 唤醒。
    - EventLoop 找到对应的 Channel，调用 Channel::handleEvent()。
    - Channel 发现是读事件，调用 readCallback。
        - Day 8：回调是 Connection::echoRead。
            - 直接 read(fd, buf, 1024)。
            - 直接 write(fd, buf) 回显。
        - Day 9：回调是 Connection::handleRead。
            - 调用 inputBuffer_.readFd(fd)。
            - 这里有一个小优化：它准备了 64KB 的栈空间，利用 readv 一次性尽可能多地读入数据。
            - 如果数据很少（"Hello"），直接进了 vector<char>。
            - 如果数据很多，溢出到栈上，然后 append 进 vector（自动扩容）。
    - 数据现在安稳地躺在 inputBuffer_ 里了。
    - Connection 从 inputBuffer_ 里取出字符串，打印出来。
2. 写流程 (Outgoing)
    - 触发点：业务逻辑决定要发数据回给客户端。
        - Day 8：直接调用 write(fd, "Hello", 5)。
            - 隐患：假设这时候网卡此时堵住了（TCP 发送窗口为 0）
            - write 返回 -1 (EAGAIN) 或者只发了 2 个字节。
            - 剩下的 "llo" 怎么办？
            - Day 8 的代码直接忽略了，这就叫丢包（应用层丢包）。
        - Day 9：调用 Connection::send("Hello")。
            - Try Write: 如果 outputBuffer_ 是空的，说明之前没积压，直接尝试 write。
            - Buffer Append: 假设只写出去了 "He" (2 bytes)，还剩 "llo"。
            - 代码把 "llo" (3 bytes) 追加到 outputBuffer_。
            - Enable EPOLLOUT: 关键一步！调用 channel_->enableWriting()。
            - 这就相当于告诉 Epoll：“这个 fd，一旦网卡不堵了（可写了），就是活跃状态，应当被唤醒”
3. 追写流程 (Post-Write)
    - 触发点：过了一会儿，网卡终于疏通了，内核缓冲区有了空位。
        - Epoll 通知：fd 5 可写 (EPOLLOUT)。
        - EventLoop -> Channel::handleEvent -> Channel::writeCallback。
    - Day 9 新增：回调是 Connection::handleWrite。
    - 这里的逻辑很简单：只要 outputBuffer_ 还有数据，就继续 write。
    - 这次成功把剩下的 "llo" 发出去了。
    - outputBuffer_ 空了。
    - Disable EPOLLOUT: 任务完成，调用 channel_->disableWriting()。
    - 告诉 Epoll：“当前 fd 若协议栈缓冲区空闲，不算活跃状态”。
    - 如果不取消，Epoll 会疯狂通过 busy-loop 通知你“可写可写可写”，CPU 会飙升到 100%。


现在进入 Day 10：线程池（Thread Pool）。
Day 10 的目标：解放主 Reactor
    - 目前的架构是 单线程 Reactor 模式：
    - Acceptor 监听连接。
    - Connection 读数据。
    - 业务逻辑（比如计算 1+1）。
    - Connection 写数据。
    - 所有这一切都在同一个 main 线程里完成。
解决方案：
- 引入线程池。
    - 主线程（Reactor）只负责“收发数据”（IO 密集型）
    - 复杂的业务逻辑扔给工作线程池（CPU 密集型）去跑。



当前全流程调用链梳理 (Day 9 状态)
假设服务器已编译运行，客户端发起连接并发送 "Hello"。

1. 启动服务器 (Startup)
  - [Main]: 执行 new EventLoop() -> new Server(loop)。
  - [Server] 构造过程:
      - 执行 new Acceptor(loop)：
          - 创建监听 Socket (依次执行申请 socket fd -> bind 地址 -> 设置 listen)。
          - 基于监听 fd 创建 Channel(loop, listenFd)。
          - 设置读回调: setReadCallback(Acceptor::acceptConnection)。
          - 设置写回调: setWriteCallback(nullptr) (注: Acceptor 不会收到写事件)。
          - 开启监听读事件: enableReading()
              - 设置 Channel 事件类型为 EPOLLIN | EPOLLET。
              - 调用 loop->updateChannel()。
              - [EventLoop] 响应并调用 ep->updateChannel()。
              - [Epoll] 将配置注册到 epoll 实例中，即调用 epoll_ctl(EPOLL_CTL_ADD, listenFd)。
      - 设置新连接回调: Acceptor::setNewConnectionCallback(Server::newConnection) (注: 此处为解耦出的回调逻辑)。
  - [Main]: 执行 loop->loop()。
      - 进入 while(!quit) 死循环。
      - 线程阻塞在 epoll_wait，等待事件发生。

2. 收到连接请求 (New Connection)
  - [Kernel]: 三次握手完成，listenFd 变为可读状态。
  - [EventLoop]: epoll_wait 返回，activeChannels 数组被填充。
  - [EventLoop]: 遍历 activeChannels (此时仅有 listenChannel)。
      - 调用 listenChannel->handleEvent()。
      - [Channel]: 触发读事件，调用 readCallback (即 Acceptor::acceptConnection)。
          - [Acceptor] acceptConnection 执行:
              - 调用 socket->accept() 得到 clientFd。
              - 准备建立新 Channel 所需的数据 (如 new Socket(clientFd))。
              - 调用 newConnectionCallback (即 Server::newConnection)。
                  - [Server] newConnection 执行:
                      - 实例化 new Connection(loop, clientFd)。
                      - [Connection] 构造过程:
                          - 创建 Channel(loop, clientFd)。
                          - 设置读回调: setReadCallback(Connection::handleRead)。
                          - 设置写回调: setWriteCallback(Connection::handleWrite)。
                            (注: 目前业务逻辑尚未分离，被硬编码在上述回调函数中)
                          - 连接建立，开始等待读事件: enableReading()
                              - 设置 Channel 事件类型为 EPOLLIN | EPOLLET。
                              - 调用 loop->updateChannel() -> ep->updateChannel()。
                              - [Epoll] 执行 epoll_ctl(EPOLL_CTL_ADD, clientFd)。
                      - [Connection] 构造完成，返回 Connection*。
                  - [Server]: 将返回的新连接加入内部维护的 std::map<int, Connection*> connections。

3. 收到客户端请求 (Data Arrives)
  - [Kernel]: 客户端发来数据 "Hello"，clientFd 变为可读状态。
  - [EventLoop]: epoll_wait 返回，activeChannels 数组被填充。
  - [EventLoop]: 遍历 activeChannels (此时包含 clientChannel)。
      - 调用 clientChannel->handleEvent()。
      - [Channel]: 当前为 EPOLLIN 状态，进入 handleRead 分支，调用 readCallback (即 Connection::handleRead)。
          - [Connection] handleRead 执行 (业务逻辑触发点):
              - 调用 inputBuffer_.readFd(clientFd)，将数据从内核读取到 inputBuffer_。
              - 提取消息内容: msg = inputBuffer_.retrieveAllAsString()。
              - 执行回显逻辑: 调用 send(msg) -> 进入第 4 步。

4. 回送数据 (Send Response)
  - [Connection] send 执行:
      - 判断 outputBuffer_ 是否为空 且 socket 是否可写:
          - 尝试直接写入: write(clientFd, msg)。
          - 分支 A (假设一次写完): 直接返回。
          - 分支 B (假设没写完): 将剩余数据 append 到 outputBuffer_。
              - 开启监听写事件: channel_->enableWriting()。
              - [Epoll] 执行 epoll_ctl(EPOLL_CTL_MOD, clientFd, 增加 EPOLLOUT)。
  - [Connection]: send 返回 -> handleRead 返回。
  - [Channel]: handleEvent 返回。
  - [EventLoop]: 继续下一轮 loop() 循环，阻塞等待事件。
  ----- 以下为非一次性写完时的异步后续流程 -----
  - [Kernel] (稍后): socket 发送缓冲区出现空位，clientFd 变为可写状态。
  - [EventLoop]: epoll_wait 返回，触发 EPOLLOUT 事件。
  - [Channel]: 调用 clientChannel->handleEvent()，随后调用 writeCallback (即 Connection::handleWrite)。
      - [Connection] handleWrite 执行:
          - 调用 write(outputBuffer_) 发送剩余数据。
          - 分支 A (若全部发完):
              - 取消监听写事件: channel_->disableWriting() -> epoll_ctl(EPOLL_CTL_MOD, 去除 EPOLLOUT)。
              - handleWrite 返回。
          - 分支 B (若仍没发完):
              - 更新 outputBuffer_ 的剩余数据状态，等待下次事件。
              - handleWrite 返回。
  - [Channel]: handleEvent 返回。
  - [EventLoop]: 继续下一轮 loop() 循环。

5. 客户端断开 (Disconnection)
  - [Kernel]: 收到客户端的 FIN 包，clientFd 变为可读状态 (但 payload 为 0)。
  - [EventLoop]: epoll_wait 返回，activeChannels 数组被填充。
  - [EventLoop]: 遍历 activeChannels (包含 clientChannel)。
      - 调用 clientChannel->handleEvent()。
      - [Channel]: 当前为 EPOLLIN 状态，进入 handleRead 分支，调用 readCallback (即 Connection::handleRead)。
          - [Connection] handleRead 执行:
              - 调用 readFd 读取，返回值为 0 (EOF)。
              - 进入终止连接分支:
                  - 标记状态: state_ = Closed。
                  - 调用 deleteConnectionCallback (即 Server::deleteConnection)。
                      - [Server] deleteConnection 执行:
                          - 从维护容器中移除: connections.erase(clientFd)。
                          - 销毁连接: delete Connection。
                          - [Connection] 析构过程:
                              - 销毁通道: delete Channel。
                              - 销毁套接字: delete Socket -> 底层调用 close(clientFd)。
          - [Connection]: handleRead 返回。
      - [Channel]: handleEvent 返回。
  - [EventLoop]: 继续下一轮 loop() 循环。

6. 服务器关机 (Shutdown)
  - [Main]: 满足退出条件 (quit = true)，跳出 loop->loop() 的死循环。
    (注: 目前暂无信号处理逻辑，通常依靠 Ctrl+C 强杀，由 OS 回收资源)。
  - [Main] 执行资源清理:
      - 执行 delete Server -> 触发 delete Acceptor。
      - 执行 delete EventLoop -> 触发 delete Epoll -> 底层调用 close(epollFd)。
当前全流程调用链梳理 (Day 11 版本: 单 Reactor + 任务与计算线程池)
核心改变: 引入 ThreadPool。Main Loop 负责所有 IO (accept/read/write)，复杂业务逻辑封装成 Task 扔给 ThreadPool 执行。

1. 收到客户端请求 (Data Arrives)
  - [EventLoop (Main)]: 监听到 clientFd 可读。
  - [Connection]: 调用 handleRead。
      - 执行 readFd 将数据从 Kernel 读入 inputBuffer_。(IO 操作，仍在主线程)
      - [Connection]: 创建任务 Task (lambda)。
          - 注意: 此时数据已经读到了内存中。
          - 任务内容: 从 inputBuffer_ 取数据 -> 业务处理(Echo) -> 调用 send()。
          - 执行 threadPool->add(Task)。(非阻塞，瞬间完成)
  - [EventLoop (Main)]: handleRead 返回，继续处理下一个 Channel 的事件。

2. 业务处理 (Processing in Worker Thread)
  - [ThreadPool]: 某个空闲的 Worker 线程抢到了任务。
  - [Worker Thread]: 执行任务 lambda。
      - 业务计算 (如打印日志、解析协议)。
      - 调用 Connection::send(result)。(跨线程调用!)
          - 这是一个潜在的竞争点 (Race Condition)，因为主线程可能也在操作这个 Connection。
          - 但在 Day 11 的简单 Echo 场景下由于逻辑简单，暂时没暴露问题。
  - [Worker Thread]: 任务结束，线程回到空闲状态。

----------------------------------------------------------------

当前全流程调用链梳理 (Day 12 版本: 主从 Reactor 多线程)
核心改变: Reactor 分裂。Main Reactor 只负责 Accept, 将新连接分发给 Sub Reactor。每个 Sub Reactor 独占一个线程，负责其名下所有连接的 IO 和业务。

1. 启动服务器 (Startup)
  - [Main]: 创建 Main Reactor (EventLoop)。
  - [Server]: 构造函数执行。
      - new Acceptor(mainLoop)。
      - 创建 ThreadPool (假设 4 个线程)。
      - 创建 4 个 Sub Reactor (EventLoop)。
      - 循环 4 次: threadPool->add(subLoop->loop())。
          - [Worker Threads 1-4]: 开始运行 loop()，陷入 epoll_wait 死循环。
          - 此时，这 4 个线程升级为 "IO 线程"。

2. 收到连接请求 (New Connection)
  - [Main Reactor]: 收到 listenFd 可读事件。
  - [Acceptor]: accept 得到 clientFd。
  - [Server]: newConnection(clientFd) 被回调。
      - 负载均衡: int dispatchId = clientFd % 4。
      - 选取指派: subReactor = subReactors[dispatchId]。
      - **关键移交**: new Connection(subReactor, clientFd)。
          - Connection 初始化时，会创建 Channel。
          - Channel 初始化时，会调用 loop->updateChannel()。
          - 此时的 loop 是 subReactor。
          - [Sub Reactor 线程]: 将 clientFd 加入到自己的 epoll 实例中。
  - [Main Reactor]: 任务完成，继续监听下一个连接。

3. 收到客户端请求 (Data Arrives)
  - [Sub Reactor 线程]: 自己的 epoll_wait 返回 (仅负责自己分到的约 1/4 连接)。
  - [Channel]: handleEvent -> Connection::handleRead。
  - [Connection]:
      - readFd (在子线程执行)。
      - 业务逻辑 (在子线程执行)。
      - send (在子线程执行)。
  - **全程无跨线程交互**: 不需要加锁，效率极高。Connection 就像是"由于生在哪个线程，就一辈子死在哪个线程"。

4. 客户端断开 (Disconnection)
  - [Sub Reactor 线程]: readFd 返回 0。
  - [Connection]: 调用 handleClose。
  - [Server]: deleteConnection(sock) 被回调。
      - **严重隐患**: deleteConnection 在子线程被调用，但它试图去 erase 主线程 Server 持有的 connections map。
      - (这是一个 Race Condition，将在 Day 13 修复)。

---

## Day 13：工程化与线程安全

### 核心目标

Day 12 实现了主从 Reactor 多线程，但留下了三个问题：
1. `deleteConnection` 在子线程调用，却操作主线程的 `connections_` map → Race Condition。
2. 代码风格散乱，缺少统一的复制/移动禁用机制。
3. CMakeLists.txt 结构原始，不支持库复用。

### 改进一：Macros.h

引入 `Macros.h`，统一禁止所有资源持有类被复制或移动：

```cpp
#define DISALLOW_COPY(cname) \
    cname(const cname &) = delete; \
    cname &operator=(const cname &) = delete;
#define DISALLOW_MOVE(cname) \
    cname(cname &&) = delete; \
    cname &operator=(cname &&) = delete;
#define DISALLOW_COPY_AND_MOVE(cname) DISALLOW_COPY(cname); DISALLOW_MOVE(cname);
```

为什么需要这个？因为 `Epoll`、`Connection`、`EventLoop` 等类都持有 fd（文件描述符）资源。如果允许复制，析构时会 `close` 同一个 fd 两次，引发未定义行为。

### 改进二：eventfd + queueInLoop（跨线程任务投递）

这是 Day 13 最核心的改动。问题的本质：

- `connections_` map 属于**主线程（Main Reactor）**
- 断开连接事件由**子线程（Sub Reactor）**的 `epoll_wait` 检测到
- 子线程直接 `erase` 主线程的 map → 数据竞争

解决方案：不让子线程直接操作 map，而是让它向主线程投递一个任务，让主线程自己去删。

机制：`EventLoop` 增加一个任务队列 `pendingFunctors_`，任何线程都可以向里面 `push_back`，主线程的 `loop()` 在每轮事件处理完毕后，统一执行队列里的所有任务。

问题：如果 `pendingFunctors_` 里刚放了任务，但主线程正阻塞在 `epoll_wait`（没有 IO 事件）怎么办？

解决：`eventfd`。这是一个专门用于线程间通知的 fd。`EventLoop` 在初始化时创建一个 `eventfd`，并把它注册到自己的 epoll 里。每当有任务被投进来，就向这个 `eventfd` 写一个值（`write`），这会立刻让 `epoll_wait` 返回，主线程随即执行 `doPendingFunctors()`。

```
子线程检测到断开
    → deleteConnection(sock)
        → mainReactor_->queueInLoop(task)   // 把删除任务投进主线程队列
            → 向 eventfd 写 1               // 唤醒主线程的 epoll_wait
                → 主线程 loop() 执行 doPendingFunctors()
                    → 从 connections_ map 中安全删除
```

### 改进三：CMake 静态库

将所有 `common/*.cpp` 编译为静态库 `NetLib`，各可执行文件链接 `NetLib`，避免重复编译公共代码。

---

当前全流程调用链梳理 (Day 13 版本: 线程安全的连接关闭)

4. 客户端断开 (Disconnection)
  - [Sub Reactor 线程]: epoll_wait 返回，clientFd 可读但 readFd 返回 0 (EOF)。
  - [Connection::doRead()]: 检测到 n == 0，将 state_ 设为 kClosed，调用 close()。
  - [Connection::close()]: 调用 deleteConnectionCallback_(sock_)，即 Server::deleteConnection。
  - [Server::deleteConnection (在子线程执行)]:
      - **不再直接操作 map**，而是构造一个 lambda 任务 task = { erase + delete Connection }。
      - 调用 mainReactor_->queueInLoop(task)。
          - 将 task push 进 pendingFunctors_ (加锁保护)。
          - 向 evtfd_ (eventfd) 写 1，触发主线程的 epoll_wait 返回。
  - [Main Reactor 线程]: epoll_wait 被 eventfd 唤醒，在 evtChannel_ 的读回调 handleWakeup() 中读走该值 (清零)。
  - [Main Reactor 线程]: loop() 的末尾执行 doPendingFunctors():
      - 从 pendingFunctors_ swap 出任务列表 (短暂加锁)。
      - 执行 task：erase connections_[sockfd]，delete conn。
      - **全程在主线程执行，无竞争**。

---

## Day 14：状态机 + 业务框架完全解耦

### 核心目标

Day 13 之前，`Connection` 的读回调硬编码了 Echo 业务逻辑（读到数据就原样回发）。这意味着要换业务，就必须修改 `Connection.cpp`，框架和业务混在一起，完全无法复用。

Day 14 的目标：让用户（在这个项目中就是指的 server.cpp 这个代码文件）在 `main()` 里注入一个 lambda，服务器框架永远不需要知道业务是什么。

### 改进一：Connection 状态机

`Connection` 引入 `State` 枚举，明确描述连接的当前状态：

```
kInvalid → (构造完成) → kConnected → (读返回0) → kClosed
                                   → (读返回<0) → kFailed
```

业务回调里可以通过 `conn->getState()` 判断状态，决定要不要处理数据，还是要关闭连接：

```cpp
// server.cpp（业务层，不属于框架）
server->onConnect([](Connection *conn) {
    if (conn->getState() != Connection::State::kConnected) {
        conn->close();
        return;
    }
    // 处理数据...
});
```

### 改进二：onConnect() 注入业务

`Server` 新增 `onConnect(fn)` 方法，将用户提供的 lambda 存储为 `onConnectCallback_`。当有新连接时，这个 callback 被传给 `Connection::setOnConnectCallback`，Connection 的读回调直接调用它。

这样，`main()` 里的一个 lambda 就决定了服务器的全部行为，框架代码完全不感知业务。

### 调用链变化

```
Channel::setReadCallback ← Connection::handleRead (硬编码 Echo)   [Day 13 及之前]
                         ↓
Channel::setReadCallback ← Connection 的 handleRead 调用 onConnectCallback_  [Day 14]
                                                        ↑
                                          server.cpp 里注入的 lambda
```

---

当前全流程调用链梳理 (Day 14 版本: 业务逻辑完全解耦)

1. 服务器启动 (Startup)
  - [Main]: new Eventloop() → new Server(loop)。
  - [server.cpp]: 调用 server->onConnect(lambda)，将业务 lambda 存入 Server::onConnectCallback_。
  - [Main]: loop->loop()，阻塞在 epoll_wait。

2. 新连接到来 (New Connection)
  - [Acceptor]: accept 得到 clientFd。
  - [Server::newConnection()]:
      - new Connection(subReactor, clientFd)。
      - conn->setOnConnectCallback(onConnectCallback_)：
          - Connection 内部将 channel_ 的读回调绑定为 [this]{ onConnectCallback_(this); }。
      - state_ 在 Connection 构造函数末尾被设为 kConnected。

3. 数据到来 (Data Arrives)
  - [Sub Reactor]: epoll_wait 返回，在 clientFd 上检测到 EPOLLIN。
  - [Channel::handleEvent()]: 调用 readCallback。
  - [Connection::handleRead()]:
      - 调用 inputBuffer_.readFd(fd) 读入数据，n > 0。
      - 调用 onConnectCallback_(this)，即用户注入的 lambda。
          - lambda 检查 conn->getState() == kConnected → true。
          - 调用 conn->readBuffer()->retrieveAllAsString() 取出数据。
          - 调用 conn->send(msg) 回发。

4. 客户端断开 (Disconnection)
  - [Connection::handleRead()]: readFd 返回 0，state_ = kClosed，调用 onConnectCallback_(this)。
  - [用户 lambda]: conn->getState() != kConnected → 调用 conn->close()。
  - [Connection::close()]: 调用 deleteConnectionCallback_(sock_)。
  - [Server::deleteConnection()]: queueInLoop 投递删除任务到主线程（同 Day 13）。

---

## Day 15：进一步完善业务解耦 + 工具类补充

### 核心目标

Day 14 只有一个回调 `onConnectCallback_`，它要同时处理三种情形：数据到来、连接关闭、连接失败。这三种情形的处理逻辑性质不同，混在一个 lambda 里会导致 `if/else if/else` 链的迷宫。

Day 15 将回调职责**精确拆分**为两个：

| 回调 | 触发时机 | 当前状态 | 典型用途 |
|---|---|---|---|
| `newConnectCallback_` | 新连接建立，一次性触发 | `kConnected` | 打印日志、初始化会话 |
| `onMessageCallback_` | 每次有数据可读时触发 | `kConnected` | 处理消息、回发数据 |

同时，Connection 内部的 IO 读写逻辑被彻底从回调中剥离：

### 改进一：doRead / doWrite 与 Business()

原来：`handleRead()` = 读 IO + 调业务回调（混在一起）

现在：
- `doRead()` = **纯 IO**，只负责将数据读入 `inputBuffer_`，遇到断开则调 `close()`，不调任何业务回调。
- `doWrite()` = **纯 IO**，只负责将 `outputBuffer_` 的数据发出去。
- `Business()` = `Read()` + `onMessageCallback_(this)`，是 Channel 的 read callback。

`setOnMessageCallback(fn)` 被调用后，Channel 的读回调从 `doRead` 换成 `Business`：

```cpp
void Connection::setOnMessageCallback(fn) {
    onMessageCallback_ = fn;
    channel_->setReadCallback(std::bind(&Connection::Business, this));
    //      ↑ 注册后，每次 epoll 通知读事件，触发链路变为：
    //        Channel → Business() → doRead() + onMessageCallback_(this)
}
```

这样框架和业务的边界完全清晰：框架只调 `Business()`，业务只看 `onMessageCallback_`。

### 改进二：三个工具头文件

**`Exception.h`**：定义 `Exception` 类继承自 `std::runtime_error`，携带 `ExceptionType` 枚举。替代原来的 `ErrorIf(cond, msg)` 宏，在条件不满足时抛出异常，调用栈清晰，可被上层捕获。

**`SignalHandler.h`**：用 `inline` 全局 map + 函数封装 `::signal()`，让信号处理变成：
```cpp
Signal::signal(SIGINT, [&]{ loop->setQuit(); });
```
消除了 `server.cpp` 里以前那个全局 `g_loop` 变量和 `signalHandler` 函数，代码更内聚。

**`pine.h`**：一行 include 入口，引入所有框架头文件，简化使用方的 include 列表。

---

当前全流程调用链梳理 (Day 15 版本: 双回调 + 纯 IO 分离)

1. 服务器启动 (Startup)
  - [Main]: new Eventloop() → new Server(loop)。
  - [server.cpp]: 注册两个回调：
      - server->newConnect(lambda_A)：新连接通知（一次性）。
      - server->onMessage(lambda_B)：消息处理（每次数据到来）。
  - [Main]: loop->loop()，阻塞在 epoll_wait。

2. 新连接到来 (New Connection)
  - [Acceptor]: accept → clientFd。
  - [Server::newConnection()]:
      - new Connection(subReactor, clientFd)：构造函数内：
          - channel_ 的读回调初始被绑定为 Connection::doRead（仅做 IO，尚无业务）。
          - state_ = kConnected。
      - conn->setOnMessageCallback(onMessageCallback_)：
          - 将 lambda_B 存入 onMessageCallback_。
          - **重绑 channel 读回调** 为 Connection::Business。
      - if (newConnectCallback_) newConnectCallback_(conn)：
          - 调用 lambda_A，打印"新连接"日志等。
          - 注意：此时 channel 读回调已经是 Business，newConnectCallback_ 只是一次性通知，不参与数据流。

3. 数据到来 (Data Arrives)
  - [Sub Reactor]: epoll_wait 返回，EPOLLIN 触发。
  - [Channel::handleEvent()]: 调用 readCallback，即 Connection::Business()。
  - [Connection::Business()]:
      - 调用 Read()，即 doRead()：
          - inputBuffer_.readFd(fd)：将数据读入 inputBuffer_。
          - n > 0：什么都不做，返回给 Business()。
          - n == 0：state_ = kClosed，调用 close()，触发断开流程，Business() 后续的 onMessageCallback_ 不会被调（doRead 里直接 return 了，但实际上 Business 还会调 onMessageCallback_，业务需自行判断 state）。
      - 调用 onMessageCallback_(this)，即 lambda_B：
          - 业务检查 conn->getState()。
          - 取数据、回发。

4. 客户端断开 (Disconnection)
  - [Connection::doRead()]: n == 0，state_ = kClosed，调用 close()。
  - [Server::deleteConnection()]: queueInLoop 投递到主线程（同 Day 13/14）。
  - 注意：doRead 调用 close() 后随即返回，随后 Business() 仍会调 onMessageCallback_。
    这意味着 **业务 lambda 有责任检查 `conn->getState() == kConnected` 后再操作**，否则会对一个已关闭的连接做无意义的读写。

5. 捕获 SIGINT 关机 (Shutdown)
  - [Signal::signal()]: SIGINT 触发 `handlers_[SIGINT]()`，即 lambda { loop->setQuit() }。
  - [Eventloop::loop()]: quit_ == true，退出 while 循环。
  - [Main]: delete server → delete loop，析构链清理所有资源。

---

## Day 16：智能指针重构 + TcpServer 自包含

### 核心目标

Day 15 之后，架构上还残留着两个隐患：

1. **裸指针所有权模糊**：
    - `Server` 用 `map<int, Connection*>` 持有连接
    - `Acceptor` 内部创建了 `Socket*` 和 `Channel*`
    - `Connection` 构造时也 `new` 
    - 它们——这些裸指针的所有权语义完全依靠程序员的自律来维持，极易漏删。
2. **Server 需要外部传入 EventLoop**：
    - `main()` 里必须先 `new EventLoop`，再 `new Server(loop)`，Server 不是自包含的。
    - 这对使用方不友好，也让 Server 的生命周期与外部 EventLoop 产生了隐式的耦合。

Day 16 的目标就是用 `unique_ptr` 把所有权收紧，并抽出一个完全自包含的 `TcpServer` 类，让使用方的 `main()` 只需要两行。

### 改进一：RC 枚举（`Macros.h`）

在 `Macros.h` 末尾追加一个返回码枚举，用于替代裸 `bool` 或 `void` 返回，让函数的成功/失败语义在类型上显式：

```cpp
enum RC {
    RC_UNDEFINED,
    RC_SUCCESS,
    RC_SOCKET_ERROR,
    RC_POLLER_ERROR,
    RC_CONNECTION_ERROR,
    RC_ACCEPTOR_ERROR,
    RC_UNIMPLEMENTED
};
```


### 改进二：Connection 改用 `unique_ptr`

**构造函数签名调整**：
    - 参数由 `Connection(Eventloop*, Socket*)` 改为 `Connection(int fd, Eventloop*)`, fd 在前。
    - `Socket` 不再由外部创建后传入，而是在 Connection 内部 `make_unique`：
        ```cpp
        Connection::Connection(int fd, Eventloop *loop)
            : loop_(loop),
            sock_(std::make_unique<Socket>(fd)),
            channel_(nullptr) {
            channel_ = std::make_unique<Channel>(loop_, sock_->getFd());
            channel_->setReadCallback(std::bind(&Connection::doRead, this));
            channel_->setWriteCallback(std::bind(&Connection::doWrite, this));
            channel_->enableReading();
            channel_->enableET();
            state_ = State::kConnected;
        }
        ```

    - 析构函数变为空体 `{}`——`sock_` 和 `channel_` 作为 `unique_ptr` 成员，Connection 析构时会自动按声明逆序销毁它们，不再需要手动 delete。

**deleteConnectionCallback 签名调整**：
    - 由 `std::function<void(Socket*)>` 改为 `std::function<void(int)>`
    - `close()` 里调用时传 `sock_->getFd()` 而非 Socket 指针。
        - 原因：Server（或 TcpServer）用 fd 作为 map 的 key
        - 直接传 int 更自然，且避免 Server 层感知 Socket 对象的存在。

**头文件依赖的重要细节**：`
    - Connection.h` 中 `sock_` 和 `channel_` 都是 `unique_ptr` 成员。
    - `unique_ptr<T>` 在析构时需要调用 `T` 的析构函数，要求 `T` 是**完整类型**。
    - 仅有前向声明 `class Socket;` 是不够的——编译器在实例化 `default_delete<Socket>` 时会报 `sizeof incomplete type` 错误。
    - 因此 `Connection.h` 必须将原来的前向声明换成真正的 include：

```cpp
// 之前（错误）
class Socket;
class Channel;

// 之后（正确）
#include "Socket.h"
#include "Channel.h"
```

### 改进三：Acceptor 改用 `unique_ptr`

    - `Acceptor` 内部的 `Socket*` 和 `Channel*` 同样改为 `unique_ptr` 成员。
    - 此外，新连接回调的签名从 `std::function<void(Socket*)>` 精简为 `std::function<void(int)>`
        - Acceptor 在 `accept()` 后直接把裸 fd 传给上层，自己不再构造 Socket 对象：

```cpp
// acceptConnection 的核心逻辑：
int clientFd = sock_->accept(&clientAddr);
// 用 fcntl 设置非阻塞（不依赖 Socket 封装）
fcntl(clientFd, F_SETFL, fcntl(clientFd, F_GETFL) | O_NONBLOCK);
newConnectionCallback_(clientFd);  // 仅传 int fd
```

Acceptor 的析构函数也只需声明，不写任何 delete——`unique_ptr` 自动完成。

### 改进四：抽出自包含的 `TcpServer`

- 原来的 `Server` 类需要外部传入 `EventLoop*`
- Day 16 新建一个 `TcpServer`
    - 将 Main Reactor、Acceptor、Sub Reactors、ThreadPool 全部通过 `unique_ptr` 自持：
        ```cpp
        class TcpServer {
        private:
            std::unique_ptr<Eventloop> mainReactor_;
            std::unique_ptr<Acceptor> acceptor_;
            std::unordered_map<int, std::unique_ptr<Connection>> connections_;
            std::vector<std::unique_ptr<Eventloop>> subReactors_;
            std::unique_ptr<ThreadPool> threadPool_;
            // ...
        public:
            TcpServer();
            ~TcpServer();  // 声明在 .h，定义在 .cpp（见下）
            void Start();
            void onMessage(std::function<void(Connection *)> fn);
            void newConnect(std::function<void(Connection *)> fn);
        };
        ```
    - 构造函数内部完成所有初始化：
    ```cpp
    TcpServer::TcpServer() {
        mainReactor_ = std::make_unique<Eventloop>();
        acceptor_ = std::make_unique<Acceptor>(mainReactor_.get());
        acceptor_->setNewConnectionCallback(
            std::bind(&TcpServer::newConnection, this, std::placeholders::_1));

        int threadNum = std::thread::hardware_concurrency();
        threadPool_ = std::make_unique<ThreadPool>(threadNum);
        for (int i = 0; i < threadNum; ++i)
            subReactors_.push_back(std::make_unique<Eventloop>());
    }
    ```
    - `Start()` 启动所有 Sub Reactor 线程后，让主线程进入 Main Reactor 的 `loop()`（阻塞）：
        ```cpp
        void TcpServer::Start() {
            for (auto &sub : subReactors_)
                threadPool_->add(std::bind(&Eventloop::loop, sub.get()));
            mainReactor_->loop();
        }
        ```

    - 连接容器也从 `map<int, Connection*>` 升级为 `unordered_map<int, unique_ptr<Connection>>`
        - 断开连接时只需 `connections_.erase(fd)`
        - `unique_ptr` 自动析构 Connection 及其持有的 Socket/Channel。


    - `TcpServer` 头文件里最初写的是 `~TcpServer() = default;`。
    - 这导致编译器在**头文件里内联生成析构函数**。
    - 而此时 `Eventloop`、`Acceptor`、`ThreadPool` 均只有前向声明
    - `unique_ptr` 的 `default_delete` 实例化时遇到不完整类型，报错 `sizeof incomplete type`。
    - 修复方式：析构函数只在 `.h` 里**声明**，在 `.cpp` 里（已 `#include` 了所有完整类型）**定义**：
        ```cpp
        // TcpServer.h
        ~TcpServer();  // 仅声明

        // TcpServer.cpp
        TcpServer::~TcpServer() = default;  // 定义在此，完整类型可见
        ```

### 调用链变化

- `main()` 从原来的：
    ```cpp
    Eventloop *loop = new Eventloop();
    Server *server = new Server(loop);
    // ...注册回调...
    loop->loop();
    ```
- 简化为：
    ```cpp
    TcpServer *server = new TcpServer();
    server->newConnect([](Connection *conn) { /* ... */ });
    server->onMessage([](Connection *conn) { /* ... */ });
    server->Start();  // 内部启动所有线程，阻塞在 Main Reactor
    ```
- 原有的 `Server.h` / `Server.cpp` 随之删除
- `MyCppServerLib.h` 中的 `#include "Server.h"` 替换为 `#include "TcpServer.h"`。

---

当前全流程调用链梳理 (Day 16 版本: unique_ptr + 自包含 TcpServer)

1. 服务器启动 (Startup)
  - [Main]: new TcpServer()。
  - [TcpServer] 构造过程（全在构造函数内完成）:
      - make_unique<Eventloop>() → mainReactor_。
      - make_unique<Acceptor>(mainReactor_.get()) → acceptor_：
          - Acceptor 内部：make_unique<Socket>() 完成 bind/listen，make_unique<Channel>() 注册到 mainReactor_。
          - 读回调绑定为 Acceptor::acceptConnection。
          - enableReading() → epoll_ctl(EPOLL_CTL_ADD, listenFd)。
      - acceptor_->setNewConnectionCallback(TcpServer::newConnection)。
      - make_unique<ThreadPool>(threadNum) → threadPool_。
      - 循环 threadNum 次：subReactors_.push_back(make_unique<Eventloop>())。
  - [Main]: server->newConnect(lambda_A)，server->onMessage(lambda_B)。
  - [Main]: server->Start()。
      - 循环：threadPool_->add(subLoop->loop())。
          - [Worker Threads]: 每个子线程进入自己的 EventLoop::loop()，阻塞在 epoll_wait。
      - 主线程：mainReactor_->loop()，阻塞在 epoll_wait。

2. 新连接到来 (New Connection)
  - [Main Reactor]: epoll_wait 返回，listenFd 可读。
  - [Acceptor::acceptConnection()]:
      - accept() 得到 clientFd。
      - fcntl(clientFd, ...) 设置非阻塞。
      - 调用 newConnectionCallback_(clientFd)，即 TcpServer::newConnection(fd)。
          - [TcpServer::newConnection]:
              - 负载均衡：idx = fd % subReactors_.size()。
              - conn = make_unique<Connection>(clientFd, subReactors_[idx].get())。
                  - [Connection 构造]: make_unique<Socket>(fd)，make_unique<Channel>(subLoop, fd)。
                  - channel_ 读回调初始绑定为 Connection::doRead（纯 IO）。
                  - state_ = kConnected。
              - conn->setOnMessageCallback(onMessageCallback_)：重绑 channel 读回调为 Business。
              - conn->setDeleteConnectionCallback(TcpServer::deleteConnection)。
              - connections_[fd] = std::move(conn)：unique_ptr 所有权转入 unordered_map。
              - if (newConnectCallback_) newConnectCallback_(rawConn)：一次性通知 lambda_A。

3. 数据到来 (Data Arrives)
  - [Sub Reactor 线程]: epoll_wait 返回，clientFd 触发 EPOLLIN。
  - [Channel::handleEvent()]: 调用读回调 → Connection::Business()。
  - [Connection::Business()]:
      - doRead()：inputBuffer_.readFd(fd)，数据读入。n == 0 则调 close() 后返回。
      - onMessageCallback_(this)，即 lambda_B：业务检查 state，取数据，conn->send(msg)。

4. 客户端断开 (Disconnection)
  - [Connection::doRead()]: readFd 返回 0，state_ = kClosed，调用 close()。
  - [Connection::close()]: 调用 deleteConnectionCallback_(sock_->getFd())，即 TcpServer::deleteConnection(fd)。
  - [TcpServer::deleteConnection (在子线程执行)]:
      - 构造 task = { connections_.erase(fd) }。
      - mainReactor_->queueInLoop(task)（同 Day 13 的 eventfd 唤醒机制）。
  - [Main Reactor 线程]: 被 eventfd 唤醒，doPendingFunctors() 执行 task。
      - connections_.erase(fd)：unique_ptr 随即析构 Connection。
      - [Connection 析构]: channel_ 先析构（disableAll + 从 epoll 删除），sock_ 再析构（close(fd)）。
      - **全程无裸指针 delete，生命周期由 unique_ptr 保证**。

5. 捕获 SIGINT 关机 (Shutdown)
  - [Signal::signal()]: SIGINT 触发 lambda { delete server }。
  - [TcpServer 析构]: unique_ptr 成员按声明逆序自动析构：
      - threadPool_ 析构：等待所有工作线程退出。
      - subReactors_ 析构：每个 EventLoop 的 epoll fd 被 close。
      - acceptor_ 析构：监听 socket 被 close。
      - mainReactor_ 析构：main epoll fd 被 close。
  - **无任何手动 delete，析构链完全由 RAII 驱动**。

---

## Day 16.5：压力测试段错误根治——跨线程生命周期


- Day 16 的代码在低并发场景下运行完全正常。
- 但一旦使用 `./StressTest 100 2000` 进行压力测试，服务器必然在数秒内因段错误崩溃：
    ```
    [server] client fd 43 disconnected.
    [TcpServer] connection fd=43 deleted.
    zsh: segmentation fault  ./server
    ```
- 崩溃位置随机，但总发生在连接断开的前后。
- 这是多线程 **use-after-free** 现象。

### 原因：Connection 销毁与子 Reactor 事件循环的竞态

程序的多线程模型是：
- Main Reactor 线程负责接受新连接
- Sub Reactor 线程（IO 线程）各自运行 `loop()`
    - 在 `kevent()` 返回后遍历事件列表并调用 `Channel::handleEvent()`。
    - 崩溃的完整竞态链路如下：

        ```
        [Sub Reactor 线程]                       [Main Reactor 线程]
        kevent() 返回
        events_[i].udata = Channel A*           此时 Channel A 还活着
        遍历事件列表...
                                                doPendingFunctors() 执行删除任务
                                                connections_.erase(fd)
                                                unique_ptr<Connection> 析构
                                                Channel A 被 delete
        ch = events_[i].udata 
        ch->setReadyEvents(ev)解引用野指针
        段错误
        ```
- `kevent()` 的 `udata` 字段存储的是 `Channel*` 裸指针。
- 主线程销毁这片内存，与子线程遍历事件列表之间完全没有同步保障
- 两者并发执行就必然导致崩溃。

### 改进一：将 Connection 的析构转移到其归属的子 Reactor 线程

**问题本质：** 谁拥有 Channel 的生命周期，就应该在谁的线程里销毁它。

- Channel 登记到了子 Reactor 的 kqueue 里
    - `kevent()` 返回的事件列表里保存着指向它的裸指针。
    - 只要这个子 Reactor 线程还在当前 `loop()` 迭代里
    - （即还在遍历 `events_`）
    - 这个指针就不能失效。
- 原来的 `deleteConnection` 将删除任务投递给主线程：
    ```cpp
    // 旧实现：Connection 在主线程 erase → unique_ptr 在主线程析构 → Channel 在主线程被 delete
    auto task = [this, fd]() {
        connections_.erase(fd);  // Channel 在此被 delete，与子线程遍历 events_ 无任何同步
    };
    mainReactor_->queueInLoop(task);
    ```
- 修复思路分为两步：
1. 在**主线程**从 `connections_` map 中移除所有权（保证 map 操作的线程安全），但**不在此析构** Connection。
2. 将 Connection 裸指针的 `delete` 投递到**其归属的子 Reactor 线程**
    - 通过 `queueInLoop` 机制确保它在子线程的 `doPendingFunctors()` 阶段执行
    - 此时当前 `loop()` 迭代中的所有 `ch->handleEvent()` 早已执行完毕，
    - `events_` 数组不再引用该 Channel，可以安全释放内存。
        ```cpp
        void TcpServer::deleteConnection(int fd) {
            mainReactor_->queueInLoop([this, fd]() {
                auto it = connections_.find(fd);
                if (it != connections_.end()) {
                    Eventloop *ioLoop = it->second->getLoop();   // 取出归属的子 Reactor
                    Connection *raw = it->second.release();      // 移走所有权，not delete
                    connections_.erase(it);
                    std::cout << "[TcpServer] connection fd=" << fd << " deleted." << std::endl;

                    // 在子线程的下一次 doPendingFunctors() 里析构：
                    // 此时该迭代的所有 handleEvent 已结束，events_ 里不再持有 Channel* 的引用
                    ioLoop->queueInLoop([raw]() { delete raw; });
                }
            });
        }
        ```
    - 为此，`Connection` 新增 `getLoop()` 方法，向外暴露自己所属的 `Eventloop*`：
        ```cpp
        Eventloop *Connection::getLoop() const { return loop_; }
        ```

---

### 改进二：Connection 析构时主动从 kqueue 注销 Channel

- Channel 析构只会释放自身的内存，并不会自动向 kqueue 发出 `EV_DELETE`。
    - 这意味着即便 Connection 已经在子线程被 `delete`。
    - kqueue 内核事件表里仍然保留着一条 `udata` 指向已释放内存的陈旧记录。
    - 下一次 `kevent()` 返回时，若这个 fd 恰好被复用（OS 可能将其分配给新的连接），
    - 新 Channel 的注册会和旧记录发生混淆
    - 更危险的情况是这个 fd 在内核里还能触发事件（如对端发来 RST），引发第二次野指针解引用。

- 修复：在 `Connection` 的析构函数里，将 Channel 注销的职责显式化。
    - 在 `unique_ptr<Channel>` 成员被自动析构之前，先主动调用 `loop_->deleteChannel(channel_.get())`
    - 命令 kqueue 删除所有与该 fd 相关的 filter：
        ```cpp
        Connection::~Connection() {
            // 先通知 kqueue 删除，再让 unique_ptr 析构 Channel 本体
            loop_->deleteChannel(channel_.get());
        }
        ```
    - 为此，`Eventloop` 新增 `deleteChannel()` 转调 `Poller::deleteChannel()`：
        ```cpp
        // EventLoop.h
        void deleteChannel(Channel *ch);

        // Eventloop.cpp
        void Eventloop::deleteChannel(Channel *ch) { poller_->deleteChannel(ch); }
        ```

### 改进三：Channel 启用时机推迟到子 Reactor 线程（消除回调未就绪竞态）

- Day 16 的 `Connection` 构造函数在**主线程**（调用 `newConnection` 的 Main Reactor 线程）里直接调用了：
    - `channel_->enableReading()` 和 `channel_->enableET()`。
    - 这意味着 Channel 的 `kevent ADD` 发生在 `setOnMessageCallback`（重绑读回调为 `Business`）**之前**：
        ```
        // Day 16 的时序（错误）：
        Connection 构造函数
            enableReading() // Channel 已注册到子 Reactor 的 kqueue
            enableET()      // 子线程随时可能被唤醒
                            //构造完成
        TcpServer::newConnection
            setOnMessageCallback()  // 读回调此时才被设置
            connections_[fd] = conn // Connection 此时才进 map
        ```
- 高并发下，新连接建立后数据几乎同时到达：
    - Channel 注册完毕
    - 子线程的 `kevent()` 立刻返回
    - 但此时读回调尚未被替换为 `Business`（仍然是初始的 `doRead`），`onMessageCallback_` 也仍然是空的。
    - 轻则业务数据被丢弃，重则调用空 `std::function` 抛异常崩溃。
- 修复：
    - 将 `enableReading` 和 `enableET` 从构造函数中彻底移除
    - 仅在 `TcpServer::newConnection` **完成所有回调注册、Connection 入 map 之后**
    - 通过 `queueInLoop` 投递到子 Reactor 线程执行：
        ```cpp
        // Connection 构造函数：不再启用任何事件
        Connection::Connection(int fd, Eventloop *loop)
            : loop_(loop), sock_(std::make_unique<Socket>(fd)) {
            channel_ = std::make_unique<Channel>(loop_, sock_->getFd());
            channel_->setReadCallback(std::bind(&Connection::doRead, this));
            channel_->setWriteCallback(std::bind(&Connection::doWrite, this));
            state_ = State::kConnected;
            // enableReading / enableET 不在这里调用
        }

        // 新增方法：由子 Reactor 线程在 doPendingFunctors() 里调用
        void Connection::enableInLoop() {
            channel_->enableReading();
            channel_->enableET();
        }
        ```

        ```cpp
        // TcpServer::newConnection：所有准备工作完成后，才投递启用任务
        conn->setOnMessageCallback(onMessageCallback_);
        conn->setDeleteConnectionCallback(...);
        connections_[fd] = std::move(conn);
        if (newConnectCallback_) newConnectCallback_(rawConn);

        // 最后一步：在子 Reactor 线程里启用 Channel，此时所有状态已就绪
        subReactors_[idx]->queueInLoop([rawConn]() { rawConn->enableInLoop(); });
        ```

### 改进四：修复 KqueuePoller 的三处正确性缺陷

#### 1：updateChannel 无法真正移除事件

- 原实现对所有情况都执行 `EV_ADD | EV_ENABLE`，从不删除已有的 filter。
- 这导致调用 `disableWriting()`（清除 `WRITE_EVENT` 位后调 `updateChannel`）时：
    - kqueue 里的 `EVFILT_WRITE` 实际上无法被取消
    - 因为没有执行 `EV_DELETE`，
    - kqueue 会把新的 `EV_ADD` 理解为"更新参数"而非"替换事件集合"。
- 结果：一旦写事件被注册过，后续对 `disableWriting()` 的调用全部静默失效。
    - socket 变得可写后，`EVFILT_WRITE` 会持续触发，
    - `handleWrite()` 被反复调用
- 修复：
    - 每次 `updateChannel` 时，若 Channel 此前已在 kqueue 中
    - 先逐条 `EV_DELETE` 清除所有旧 filter，再添加当前需要的 filter：
        ```cpp
        void KqueuePoller::updateChannel(Channel *channel) {
            int fd = channel->getFd();
            int listenEvents = channel->getListenEvents();

            if (channel->getInEpoll()) {
                // 分两次独立调用：容忍某个 filter 不存在时 kevent 返回 ENOENT
                struct kevent delEv;
                EV_SET(&delEv, fd, EVFILT_READ, EV_DELETE, 0, 0, nullptr);
                kevent(kqueueFd_, &delEv, 1, nullptr, 0, nullptr);
                EV_SET(&delEv, fd, EVFILT_WRITE, EV_DELETE, 0, 0, nullptr);
                kevent(kqueueFd_, &delEv, 1, nullptr, 0, nullptr);
            }
            // 重新添加当前需要的 filter
            struct kevent ev[2];
            int n = 0;
            if (listenEvents & Channel::READ_EVENT)
                EV_SET(&ev[n++], fd, EVFILT_READ, EV_ADD | EV_ENABLE, 0, 0, channel);
            if (listenEvents & Channel::WRITE_EVENT)
                EV_SET(&ev[n++], fd, EVFILT_WRITE, EV_ADD | EV_ENABLE, 0, 0, channel);
            if (n > 0)
                ErrIf(kevent(kqueueFd_, ev, n, nullptr, 0, nullptr) == -1, "...");
            channel->setInEpoll(listenEvents != 0);
        }
        ```

#### 2：deleteChannel 批量操作导致静默失败

- 原实现将 `EVFILT_READ` 和 `EVFILT_WRITE` 的 `EV_DELETE` 合并进同一个 `kevent(ev, 2, ...)` 调用：
    ```cpp
    // 旧实现（错误）
    struct kevent ev[2];
    EV_SET(&ev[0], fd, EVFILT_READ,  EV_DELETE, 0, 0, nullptr);
    EV_SET(&ev[1], fd, EVFILT_WRITE, EV_DELETE, 0, 0, nullptr);
    kevent(kqueueFd_, ev, 2, nullptr, 0, nullptr); // 忽略错误
    ```
- kqueue 批量操作的语义是：
    - 若第一条指令执行失败（如 `EVFILT_WRITE` 未注册，返回 `ENOENT`），整个调用返回 -1
    - **后续指令是否被执行取决于内核实现**
    - 在 macOS 上实测第二条也不会被执行。
    - 这意味着：
        - 对于只注册了读事件的连接（`EVFILT_READ` 已注册，`EVFILT_WRITE` 未注册）
        - 尝试批量删除时
        - `EVFILT_WRITE` 的 `ENOENT` 会导致整个调用失败，`EVFILT_READ` 同样没有被删除
        - 野指针 `udata` 留在内核里。
- 修复：
    - 分两次独立调用，各自容忍对方的 `ENOENT`：
        ```cpp
        void KqueuePoller::deleteChannel(Channel *channel) {
            int fd = channel->getFd();
            struct kevent ev;
            EV_SET(&ev, fd, EVFILT_READ, EV_DELETE, 0, 0, nullptr);
            kevent(kqueueFd_, &ev, 1, nullptr, 0, nullptr);   // 忽略错误
            EV_SET(&ev, fd, EVFILT_WRITE, EV_DELETE, 0, 0, nullptr);
            kevent(kqueueFd_, &ev, 1, nullptr, 0, nullptr);   // 忽略错误
            channel->setInEpoll(false);
        }
        ```

#### 3：poll 对同一 Channel 重复触发

- kqueue 对同时注册了 `EVFILT_READ` 和 `EVFILT_WRITE` 的 fd：
    - 当两个事件同时就绪时，会在一次 `kevent()` 返回中产生**两条独立记录**
    - 相同的 `udata`，不同的 `filter`。
    - 原实现对每条记录都直接入列：
        ```cpp
        // 旧实现（错误）
        for (int i = 0; i < nfds; ++i) {
            Channel *ch = static_cast<Channel *>(events_[i].udata);
            ch->setReadyEvents(...);       // 第一次设置，比如 READ_EVENT
            activeChannels.push_back(ch); // 入列
        }
        // 下一条记录同一 Channel 再次入列
        // handleEvent() 被调用两次，第二次调用时 readyEvents 已被覆盖为 WRITE_EVENT
        // 第一次设置的 READ_EVENT 丢失，读回调没有被触发
        ```
- 修复：
    - 用 `std::unordered_map<Channel*, int>` 先将同一 Channel 上的所有就绪事件用位或（`|=`）合并，再统一入列：
        ```cpp
        std::unordered_map<Channel *, int> channelEvents;
        for (int i = 0; i < nfds; ++i) {
            Channel *ch = static_cast<Channel *>(events_[i].udata);
            if (events_[i].filter == EVFILT_READ)  channelEvents[ch] |= Channel::READ_EVENT;
            if (events_[i].filter == EVFILT_WRITE) channelEvents[ch] |= Channel::WRITE_EVENT;
        }
        for (auto &[ch, ev] : channelEvents) {
            ch->setReadyEvents(ev);
            activeChannels.push_back(ch);
        }
        ```

---

当前全流程调用链梳理 (Day 16.5 版本)

2. 新连接到来 (New Connection)
  - [TcpServer::newConnection]:
      - make_unique<Connection>(fd, subLoop)：构造函数不再调用 enableReading / enableET，Channel 尚未注册到 kqueue。
      - conn->setOnMessageCallback(lambda_B)：重绑 channel 读回调为 Business。
      - conn->setDeleteConnectionCallback(TcpServer::deleteConnection)。
      - connections_[fd] = std::move(conn)：Connection 已入 map，所有状态已就绪。
      - if (newConnectCallback_) newConnectCallback_(rawConn)：一次性通知 lambda_A。
      - subReactors_[idx]->queueInLoop([rawConn]() { rawConn->enableInLoop(); })：
          - [Sub Reactor 线程]: 在下一次 doPendingFunctors() 执行：
              - channel_->enableReading()，channel_->enableET()。
              - → kevent(EV_ADD, EVFILT_READ)。
              - **此时回调已全部就绪，Connection 已在 map 中，无任何竞态窗口。**

4. 客户端断开 (Disconnection)
  - [Sub Reactor 线程]: doRead() 读到 EOF，state_ = kClosed，调用 close()。
  - [Connection::close()]: 调用 deleteConnectionCallback_(fd)，即 TcpServer::deleteConnection(fd)（在子线程执行）。
  - [TcpServer::deleteConnection]: 向 mainReactor_ 投递任务（queueInLoop + eventfd 唤醒）。
  - [Main Reactor 线程]: doPendingFunctors() 执行：
      - connections_.find(fd)，取出归属的 ioLoop（该连接所属的子 Reactor）。
      - raw = it->second.release()：移走 unique_ptr 所有权，不在此析构。
      - connections_.erase(it)：从 map 中删除 key，主线程操作 map 完毕。
      - ioLoop->queueInLoop([raw]() { delete raw; })：将析构任务投递给子 Reactor。
  - [Sub Reactor 线程]: 下一轮 doPendingFunctors() 执行 delete raw：
      - [Connection 析构]: 先调 loop_->deleteChannel(channel_.get())：
          - KqueuePoller::deleteChannel：分两次独立 kevent(EV_DELETE)，各自容忍 ENOENT。
          - channel_->setInEpoll(false)。
      - unique_ptr<Channel> channel_ 析构，Channel 对象内存释放。
      - unique_ptr<Socket> sock_ 析构，close(fd)。
      - **此时子线程的当前 loop() 迭代早已结束，events_ 数组里不再持有任何指向此 Channel 的指针，内存释放完全安全。**


---

## Day 16.9：负载中 Ctrl+C 段错误修复 + StressTest 跨平台重写

- Day 16.5 修复了压力测试中服务器自身因多线程竞态而崩溃的问题
- 并引入了 `TcpServer::stop()` 实现安全关闭。
- 但本次测试发现两个新问题：
    1. **负载中 Ctrl+C 导致段错误**：
        - 无活跃连接时 Ctrl+C 能正常退出
        - 但 StressTest 运行期间按 Ctrl+C
        - 服务器以 `zsh: segmentation fault ./server` 退出，而非预期的 `[server] Caught SIGINT, shutting down.`。
    2. **StressTest 仅支持 macOS**：
        - 测试程序无法跨平台。
        - 此外代码本身没有IO复用，会首先耗尽系统资源。


### 问题一：负载中 Ctrl+C 段错误

#### 根本原因：TcpServer 成员变量析构顺序错误

- C++ 规定，类的成员变量**按声明顺序构造，按逆序析构**。
- Day 16.5 之前，`TcpServer.h` 的成员声明顺序为：
    ```cpp
    std::unique_ptr<Eventloop>                            mainReactor_;   // 第1个声明 → 最后析构
    std::unique_ptr<Acceptor>                             acceptor_;      // 第2个声明
    std::unordered_map<int, std::unique_ptr<Connection>>  connections_;   // 第3个声明
    std::vector<std::unique_ptr<Eventloop>>               subReactors_;   // 第4个声明
    std::unique_ptr<ThreadPool>                           threadPool_;    // 第5个声明 → 最先析构
    ```
- 析构顺序（逆序）：
    - 1: `threadPool_` .子 Reactor 线程停止，Join 完成
    - 2: `subReactors_` .各 Sub Reactor 的 `Eventloop` 对象被销毁，内存释放
    - 3: `connections_` .每个 `Connection::~Connection()` 调用 `loop_->deleteChannel()`

- **崩溃发生在第 3 步**：
    - `connections_` 析构时，`Connection` 析构函数通过 `loop_` 指针调用 `Eventloop::deleteChannel()`
    - 但 `loop_` 所指向的 `Eventloop` 对象已经在第 2 步被销毁
    - 这是一次**野指针解引用**导致段错误。

#### 修复方法
- 修改后的 `TcpServer.h`：
    ```cpp
    std::unique_ptr<Eventloop>                            mainReactor_;
    std::unique_ptr<Acceptor>                             acceptor_;
    // 声明顺序决定析构顺序（逆序析构）：
    // threadPool_(先析构，Join 线程) → connections_(析构 Connection，调 deleteChannel) → subReactors_(析构 Eventloop)
    std::unique_ptr<ThreadPool>                           threadPool_;
    std::unordered_map<int, std::unique_ptr<Connection>>  connections_;   // 此时 subReactors_ 仍活着
    std::vector<std::unique_ptr<Eventloop>>               subReactors_;   // 最后释放
    ```

- 析构时序如下：

    - 1. `threadPool_` 析构
        - `ThreadPool::~ThreadPool()` 设置 `stop_=true` 并 Join 所有线程
        - **子 Reactor 线程全部退出**。
    - 2. `connections_` 析构
        - 每个 `Connection::~Connection()` 调用 `loop_->deleteChannel()`
        - 此时 `subReactors_` 里的 `Eventloop` 对象**依然存活**，调用合法。
    - 3. `subReactors_` 析构
        - `Eventloop` 对象销毁，所有子 Reactor 内存释放。

### 问题二：StressTest 跨平台重写

#### 原代码存在的问题

- 1.平台限制
    - 直接使用 `kqueue` / `kevent` / `struct kevent`，无法在 Linux 上编译运行
- 2.资源泄漏
     - `cleanupClient` 中显式调用 `close(ctx->sock->getFd())
     - `Socket::~Socket()` 析构时也会调用 `close(fd_)`
     - 两次关闭同一 fd，可能意外关闭已复用该 fd 的新连接
- 3.回显判断不可靠
    - `already_read` 字段手动累加已读字节数，在 buffer层面不可靠
    - 直接用 `Buffer::readableBytes()` 则是单一真相来源
- 4.缺少 SIGPIPE 处理
    - 服务器关闭连接后客户端仍写入会收到 `SIGPIPE`，进程默认终止，干扰测试统计
- 5.缺少 EAGAIN/EINTR 处理
    - `read()` 返回 `EAGAIN` 或被信号打断时未处理，直接当作错误

#### 重写方案

- 核心思路：**两个平台的事件处理逻辑完全相同**
    - 差异只在于"如何注册/修改监听事件"和"如何从事件结构中读取 `ctx` 和事件类型"
    - 通过三个 lambda（`pollRegisterWrite` / `pollWaitRead` / `pollWaitWrite`）
    - 和四个 bool 标志（`isError` / `isEOF` / `isWrite` / `isRead`）隔离平台差异
    - 事件循环主体只写一份。
        - **平台选择（编译时）：**
            ```cpp
            #ifdef __APPLE__
            #include <sys/event.h>
            #include <sys/time.h>
            #elif defined(__linux__)
            #include <sys/epoll.h>
            #else
            #error "Unsupported platform"
            #endif
            ```
        - **状态机上下文：**
            ```cpp
            struct ClientContext {
                int id;
                Socket *sock;
                InetAddress *addr;
                Buffer *readBuffer;
                int target_msgs;
                int current_msg_idx;
                string current_msg_str;  // WRITE 时设置，READ 时用于校验回显
                bool connected;          // 首次 WRITE 事件时通过 SO_ERROR 确认连接成功
            };
            ```
            - 去除了 `already_read`，统一用 `readBuffer->readableBytes()` 判断是否收齐。
        - **cleanupClient 修复（去掉显式 close）：**
            ```cpp
            void cleanupClient(ClientContext *ctx) {
                delete ctx->sock;      // Socket::~Socket() 负责 close(fd_)
                delete ctx->addr;
                delete ctx->readBuffer;
                delete ctx;
            }
            ```

- **事件循环关键逻辑：**
    - WRITE 事件触发：
        - 若 !connected → getsockopt(SO_ERROR) 
            - 确认连接
            - connected = true
        - 构造消息字符串
            - write() 发送
        - 发送成功
            - pollWaitRead（切换监听 READ）
        - EAGAIN
            - 保持 WRITE 监听，等可写时重试

- READ 事件触发：
    - read()
        - 追加到 readBuffer    
        - readBuffer.readableBytes() >= target_len
        - retrieveAsString 校验
        - 校验通过且还有消息
            - pollWaitWrite（切换监听 WRITE，发下一条）
        - 所有消息发完
            - cleanupClient，active_clients--
        - 收到 EOF (nr==0) 或错误
            - cleanupClient，active_clients--
        - EAGAIN/EINTR
            - 继续监听 READ


---

## Day 17.5：最终架构梳理——全流程调用链与生命周期分析

> 本节记录 Day 17 完成后（含 RAII 重构、Ctrl+C 析构顺序修复、StressTest 跨平台重写）的项目**最终稳定态**。
> 涵盖：服务器全流程调用链、所有模块的所有权关系、初始化顺序、析构顺序、以及线程安全分析。

---

### 一、模块全景与所有权树

```
main()（栈上）
└── TcpServer server（栈对象，RAII 驱动整棵树）
    ├── unique_ptr<Eventloop>  mainReactor_         ← 主 Reactor，运行在主线程
    │   └── unique_ptr<Poller> poller_              ← KqueuePoller / EpollPoller
    │       └── vector<kevent/epoll_event> events_  ← 内核事件缓冲区（std::vector，自动管理）
    │   └── unique_ptr<Channel> evtChannel_         ← 监听唤醒管道/eventfd
    │   └── int wakeupReadFd_ / wakeupWriteFd_      ← OS 资源，析构时手动 close()
    │   └── vector<function<>> pendingFunctors_      ← 跨线程任务队列（mutex 保护）
    │
    ├── unique_ptr<Acceptor>   acceptor_
    │   └── unique_ptr<Socket> sock_                ← 监听 socket（bind/listen）
    │   └── unique_ptr<Channel> channel_             ← 监听 fd 的 Channel，注册到 mainReactor_
    │
    ├── unique_ptr<ThreadPool> threadPool_           ← 管理 N 个工作线程
    │
    ├── vector<unique_ptr<Eventloop>> subReactors_   ← N 个子 Reactor，每个运行在一个工作线程
    │   └──（结构同 mainReactor_，各自独立的 poller/evtChannel/唤醒机制）
    │
    └── unordered_map<int, unique_ptr<Connection>> connections_
        └── Connection（每个活跃连接一个实例）
            ├── unique_ptr<Socket>  sock_            ← 客户端 fd 封装
            ├── unique_ptr<Channel> channel_         ← 注册到归属的子 Reactor
            ├── Buffer inputBuffer_                  ← 读缓冲（值成员，随 Connection 析构）
            └── Buffer outputBuffer_                 ← 写缓冲（值成员，随 Connection 析构）
```

**所有权规则**：每个对象由且仅由它的 `unique_ptr` 持有者负责销毁。`Channel*` 和 `Connection*` 作为观察者指针在多处传递（如 kqueue 的 `udata`、`Channel::handleEvent` 里的 `this`），但**从不拥有所有权**。

---

### 二、初始化顺序（构造阶段）

#### 1. `TcpServer` 构造函数（在 `main()` 栈帧中，主线程执行）

```
[主线程，main()]

① mainReactor_ = make_unique<Eventloop>()
      └── Eventloop()：
          - Poller::newDefaultPoller(this) → make_unique<KqueuePoller>(this)
                └── KqueuePoller()：kqueue() 创建内核队列，events_ vector 预分配 1024 槽位
          - pipe(pipeFds)：创建唤醒管道（macOS）或 eventfd(Linux)
          - evtChannel_ = make_unique<Channel>(this, wakeupReadFd_)
          - evtChannel_->setReadCallback(handleWakeup)
          - evtChannel_->enableReading() → updateChannel → kevent(EV_ADD, EVFILT_READ)

② acceptor_ = make_unique<Acceptor>(mainReactor_.get())
      └── Acceptor()：
          - make_unique<Socket>()：socket() → setsockopt(SO_REUSEADDR) → bind(8888) → listen()
          - make_unique<Channel>(mainReactor_.get(), listenFd)
          - channel_->setReadCallback(Acceptor::acceptConnection)
          - channel_->enableReading() → mainReactor_->updateChannel → kevent(EV_ADD, EVFILT_READ)
      └── acceptor_->setNewConnectionCallback(TcpServer::newConnection)

③ threadNum = hardware_concurrency()（macOS 通常为 CPU 核数）
   threadPool_ = make_unique<ThreadPool>(threadNum)
      └── ThreadPool()：构造 N 个工作线程，各自阻塞在条件变量等待任务

④ for i in [0, threadNum)：
      subReactors_.push_back(make_unique<Eventloop>())
      ── 每个 Eventloop() 构造过程同 ①，独立的 kqueue fd 和唤醒管道
```

此时所有模块已初始化，但子 Reactor 的 `loop()` 尚未启动，工作线程阻塞在 ThreadPool 的任务队列上。

#### 2. 回调注册（用户代码，在 `main()` 中）

```
server.newConnect(lambda_A)  → TcpServer::newConnectCallback_ = lambda_A
server.onMessage(lambda_B)   → TcpServer::onMessageCallback_  = lambda_B
```

#### 3. `TcpServer::Start()`（阻塞，直到 stop() 被调用）

```
for sub in subReactors_：
    threadPool_->add(bind(&Eventloop::loop, sub.get()))
    ── 工作线程从条件变量唤醒，进入 Eventloop::loop()，阻塞在 kevent/epoll_wait

主线程：
    mainReactor_->loop()  ← 阻塞在此，直到 mainReactor_->setQuit() 后返回
```

---

### 三、全流程调用链（稳定运行阶段）

#### 场景 A：新连接到来

```
① [内核] 客户端发起 connect 请求 → 监听 socket 变为可读

② [主线程 / Main Reactor]
   kevent() 返回，events_[i].udata = Acceptor 的 listenFd Channel*
   KqueuePoller::poll() 整合事件 → Channel::setReadyEvents(READ_EVENT)
   Eventloop::loop()：ch->handleEvent() → Channel 的 readCallback()
   即：Acceptor::acceptConnection()
       - accept() → clientFd
       - fcntl(clientFd, F_SETFL, O_NONBLOCK)
       - 调用 newConnectionCallback_(clientFd)
       即：TcpServer::newConnection(fd)

③ [TcpServer::newConnection，主线程]
   idx = fd % subReactors_.size()            ← 轮询负载均衡，按 fd 取模
   conn = make_unique<Connection>(fd, subReactors_[idx].get())
       └── Connection()：
           - sock_ = make_unique<Socket>(fd)
           - channel_ = make_unique<Channel>(subLoop, fd)
           - channel_->setReadCallback(Connection::doRead)    ← 初始绑定（纯 IO）
           - channel_->setWriteCallback(Connection::doWrite)
           - state_ = kConnected
   conn->setOnMessageCallback(onMessageCallback_)   ← 绑定业务回调，同时将 readCallback 切换为 Business()
   conn->setDeleteConnectionCallback(TcpServer::deleteConnection)
   rawConn = conn.get()
   connections_[fd] = move(conn)                    ← unique_ptr 所有权转入 map
   if (newConnectCallback_) newConnectCallback_(rawConn)   ← 触发一次性通知 lambda_A
   subReactors_[idx]->queueInLoop([rawConn](){ rawConn->enableInLoop(); })
       └── 将{enableInLoop}任务入队，并 write(wakeupWriteFd_) 唤醒子 Reactor

④ [子 Reactor 线程，下一次 kevent() 返回后]
   doPendingFunctors() 执行：rawConn->enableInLoop()
       - channel_->enableReading()  → subLoop->updateChannel → kevent(EV_ADD, EVFILT_READ)
       - channel_->enableET()       → kevent 更新标志加 EV_CLEAR（若启用 ET）
   此时回调全部就绪，Connection 已在 map 中，Channel 已注册到 kqueue——无任何竞态窗口
```

#### 场景 B：数据到来（回显）

```
① [内核] 客户端发来数据 → clientFd 变为可读

② [子 Reactor 线程]
   kevent() 返回，events_[i].udata = Connection 的 clientFd Channel*
   KqueuePoller::poll()：同一 fd 的 EVFILT_READ / EVFILT_WRITE 事件用 unordered_map 合并
   Eventloop::loop()：ch->handleEvent()
   → channel_ 的 readCallback() = Connection::Business()

③ [Connection::Business()]
   doRead()：
       - inputBuffer_.readFd(fd, &savedErrno)  ← 循环读入内核缓冲区所有数据
       - 若 n == 0：state_ = kClosed（对端关闭，不在此触发删除，由 Business() 统一处理）
       - 若 n < 0 且非 EAGAIN：state_ = kFailed
   onMessageCallback_(this) 即 lambda_B：
       - conn->getState() != kConnected → 调用 conn->close()，return
       - msg = inputBuffer_.retrieveAllAsString()
       - conn->send(msg)

④ [Connection::send(msg)]
   若 outputBuffer_ 为空且内核发送缓冲未满：
       直接 ::write(fd, msg)，一次性写出
   若写不完（内核缓冲满）：
       剩余数据追加到 outputBuffer_
       channel_->enableWriting() → kevent(EV_ADD, EVFILT_WRITE)

⑤ [EVFILT_WRITE 触发，子 Reactor 线程]
   ch->handleEvent() → writeCallback = Connection::doWrite()
       outputBuffer_ 剩余数据 ::write(fd)
       若全部写完：channel_->disableWriting() → kevent(EV_DISABLE, EVFILT_WRITE)
```

#### 场景 C：连接断开

```
① doRead() 返回 n == 0，state_ = kClosed
   Business() 检测到 state_ != kConnected，调用 conn->close()

② [Connection::close()]
   deleteConnectionCallback_(sock_->getFd())
   即：TcpServer::deleteConnection(fd)（在子 Reactor 线程执行）

③ [TcpServer::deleteConnection]
   向 mainReactor_ 投递任务（queueInLoop + wakeup）：
       task = [this, fd]() {
           it = connections_.find(fd)          ← 在主线程操作 map，线程安全
           ioLoop = it->second->getLoop()      ← 记录归属的子 Reactor
           conn = move(it->second)             ← 从 map 取出 unique_ptr 所有权，不在此析构
           connections_.erase(it)              ← map 删除 key
           raw = conn.release()               ← unique_ptr 放弃所有权，raw 成为裸指针
           ioLoop->queueInLoop([raw]() { delete raw; })
                                               ← 析构任务投递到子 Reactor 线程
       }

④ [主 Reactor 线程，doPendingFunctors()]
   task 执行：map 操作完毕，析构任务已入队到子 Reactor

⑤ [子 Reactor 线程，下一次 doPendingFunctors()]
   此时当前 poll() 返回的所有事件已处理完毕，events_ 数组不再引用该 Channel
   delete raw 执行：
       └── Connection::~Connection()：
           loop_->deleteChannel(channel_.get())
               ── KqueuePoller::deleteChannel：
                  kevent(EV_DELETE, EVFILT_READ)   ← 容忍 ENOENT
                  kevent(EV_DELETE, EVFILT_WRITE)  ← 容忍 ENOENT
                  channel_->setInEpoll(false)
           channel_（unique_ptr）析构：Channel 对象内存释放
           sock_（unique_ptr）析构：Socket::~Socket() → close(fd)
       inputBuffer_、outputBuffer_ 随 Connection 值成员自动析构
```

#### 场景 D：Ctrl+C 优雅关闭（负载中）

```
① [任意线程] SIGINT 到达，Signal::signal 注册的 handler 执行
   atomic_flag fired：test_and_set() 保证幂等，防止多线程重复触发
   std::cout << "[server] Caught SIGINT, shutting down."
   server.stop() 被调用

② [TcpServer::stop()]
   for sub in subReactors_：
       sub->setQuit()      ← atomic<bool> quit_ = true
       sub->wakeup()       ← write(wakeupWriteFd_, 'w')，唤醒阻塞中的 kevent
   mainReactor_->setQuit()
   mainReactor_->wakeup()

③ [各子 Reactor 线程]
   kevent() 因唤醒管道可读而返回
   Eventloop::loop() 检测 quit_ == true → 退出 while 循环
   工作线程函数返回，回到 ThreadPool 的任务队列等待

④ [主 Reactor 线程]
   mainReactor_->loop() 的 kevent() 被唤醒
   quit_ == true → 退出循环
   Start() 返回，main() 继续执行

⑤ [main() 栈帧]
   server 对象离开作用域（或 Start() 返回后紧跟 return 0）→ TcpServer::~TcpServer()
   ~TcpServer() 首先调用 stop()（幂等，再调一次无害）
   随后 unique_ptr 成员按声明逆序自动析构（详见第四节）
```

---

### 四、析构顺序与生命周期安全分析

C++ 规定：类成员变量**按声明顺序构造，按逆序析构**。
`TcpServer.h` 的声明顺序（及对应的析构逆序）：

```
声明顺序                          析构顺序（逆序）      析构时 subReactors_ 是否存活？
─────────────────────────────────────────────────────────────────────────────
① mainReactor_                    ⑤ 最后析构           n/a（主 Reactor）
② acceptor_                       ④ 第4析构            是（但线程已停）
③ subReactors_（vector）           ③ 第3析构            ─── 此时才销毁 ───
④ connections_（unordered_map）    ② 第2析构            ✓ subReactors_ 还活着！
⑤ threadPool_                     ① 最先析构            ─── 线程在此 join ───
```

**关键时序**：

**步骤 ①：`threadPool_` 析构**
- `ThreadPool::~ThreadPool()`：`stop_ = true`，条件变量广播，所有工作线程 `join()`
- 工作线程在本次 `doPendingFunctors()` 执行完后退出（因为 `loop()` 前一步已因 `quit_` 退出）
- **保证**：此后无任何工作线程在运行。子 Reactor 的 `loop()` 均已返回。

**步骤 ②：`connections_` 析构（`unordered_map<int, unique_ptr<Connection>>`）**
- map 析构遍历每个 `unique_ptr<Connection>`，调用 `Connection::~Connection()`
- `Connection::~Connection()` 调用 `loop_->deleteChannel(channel_.get())`
  - `loop_` 指向某个 `subReactors_[i].get()`
  - **此时 `subReactors_` 中的 `Eventloop` 对象仍然存活**（步骤 ③ 尚未执行）
  - `deleteChannel` → `KqueuePoller::deleteChannel` → `kevent(EV_DELETE, ...)`（内核清理）
  - `channel_->setInEpoll(false)`
- `channel_`（`unique_ptr`）接着自动析构（Channel 对象内存释放）
- `sock_`（`unique_ptr`）析构：`close(fd)`
- **线程安全**：工作线程已在步骤 ① 全部 join，没有并发执行子 Reactor 的 `loop()`，`deleteChannel` 无竞态风险

**步骤 ③：`subReactors_` 析构（`vector<unique_ptr<Eventloop>>`）**
- 每个 `Eventloop` 析构：
  - `evtChannel_`（`unique_ptr<Channel>`）先析构：Channel 内存释放
  - `poller_`（`unique_ptr<Poller>`）后析构：KqueuePoller 析构 → `close(kqueueFd_)`，`events_`（vector）自动释放
  - `close(wakeupReadFd_)`、`close(wakeupWriteFd_)`（OS 管道资源手动释放）
- **安全**：步骤 ② 中所有 Connection 已完成 `deleteChannel`，kqueue 内无残留 `udata` 野指针

**步骤 ④：`acceptor_` 析构**
- `unique_ptr<Channel>` 析构：`disableAll()` → `deleteChannel` → 从 mainReactor_ 的 kqueue 移除监听 fd
- `unique_ptr<Socket>` 析构：`close(listenFd)`
- **安全**：mainReactor_ 仍存活（步骤 ⑤ 才析构）

**步骤 ⑤：`mainReactor_` 析构（最后）**
- 同步骤 ③ 的单个 Eventloop 析构
- `close(kqueueFd_main)`，关闭主 kqueue 实例

---

### 五、各模块裸指针使用一览（日志截止日的最终状态）

| 位置 | 指针 | 类型 | 说明 |
|---|---|---|---|
| `Eventloop::loop_` 成员（无） | — | — | `Eventloop` 不持有裸指针成员 |
| `Channel::loop_` | `Eventloop *` | 观察者 | 不持有所有权，Channel 不负责销毁 Eventloop |
| `Channel::fd_` | `int` | 句柄 | 不是指针，fd 的所有权归 Socket |
| `Connection::loop_` | `Eventloop *` | 观察者 | 指向归属的子 Reactor，Connection 不负责销毁 |
| `Poller::ownerLoop_` | `Eventloop *` | 观察者 | 指向所属 Eventloop，Poller 不负责销毁 |
| `KqueuePoller::kqueueFd_` | `int` | OS 资源 | 析构时手动 `close()`，不适用 RAII |
| `Eventloop::wakeupReadFd_/WriteFd_` | `int` | OS 资源 | 析构时手动 `close()` |
| `kevent.udata` / `epoll_event.data.ptr` | `Channel *` | 内核侧观察者 | 由 `deleteChannel` 保证在 Channel 析构前从内核移除 |
| `TcpServer::deleteConnection` 中 `raw` | `Connection *` | 转移中的裸指针 | `conn.release()` 后立即投递到 `queueInLoop([raw](){ delete raw; })`，生命周期可追踪 |

**结论**：项目中所有持有所有权的指针均已改为 `unique_ptr`，或以 `std::vector` 替代 `new[]`/`delete[]`。裸指针仅在以下两种场景合法出现：
1. **观察者**（不持有所有权，生命周期由持有者保证先于观察者结束）
2. **OS 文件描述符**（int 类型，手动 `close()`，不适合 RAII 封装的情形注明在代码注释中）

---

### 六、线程安全分析（最终状态）

| 共享资源 | 访问路径 | 保护机制 |
|---|---|---|
| `connections_`（map） | 主线程写（insert）；`deleteConnection` 的 task 也由主线程执行（queueInLoop 投递） | 逻辑隔离：所有 map 操作均在主 Reactor 线程执行，无需 mutex |
| `pendingFunctors_`（各 Eventloop） | 任意线程 push（queueInLoop）；本体线程 swap 执行 | `std::mutex mutex_` 保护；swap-and-execute 模式减少锁持有时间 |
| `quit_`（Eventloop） | 任意线程写（stop()）；本体线程读（loop()） | `std::atomic<bool>`，无需 mutex |
| SIGINT 处理函数 | 任意线程执行 | `std::atomic_flag fired`（test_and_set）保证幂等，防止多线程重入 |
| `Channel::handleEvent()` 过程中的 Channel* | 子 Reactor 线程读；主线程写（queueInLoop 投递 delete） | 时序保证：delete 通过 queueInLoop 投递到子线程，在下一次 doPendingFunctors()（即下一次 poll() 之后）执行，当前 events_ 遍历已完成 |
