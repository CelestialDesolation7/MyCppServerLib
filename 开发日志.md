# MyCppServerLib 开发日志

服务器的生命周期如下：


- socket: 创建套接字。
    - 这是一个整数句柄，指向一个内核中的结构体
- bind: 绑定 IP 和端口（这相当于实例化了这个 socket 的存在本身）。
- listen: 监听（这会让 OS 将这个 socket 标记为监听类 socket）。
- accept: 接收连接（这是一个阻塞操作，程序会停在这里直到有客户端连上来）。
- read/write: 通信


socket 分为两种类型：
- 监听socket：用于监听客户端连接的套接字。（永远不会有目的端口，只是为了处理建立新连接的请求）
    - 它具有两个队列：
        - 已完成连接队列：存储已经完成三次握手的连接，等待服务器调用 `accept` 来处理。
        - 半连接队列：存储正在进行三次握手的连接，等待完成后进入已完成连接队列。
        - 握手过程是 OS 的责任，服务器程序不需要关心
- 连接socket：用于与客户端通信的套接字。（含有完整的五元组：源IP、源端口、目的IP、目的端口、协议）
    - 它具有发送和接受缓冲区。


IO复用：一个线程（服务器线程，复用者）同时监控多个
socket_fd（文件描述符，对应于多个IO事件）


Epoll的生命周期如下：
- 创建epoll实例：调用 `epoll_create` 创建一个epoll实例。
- 注册文件描述符：使用 `epoll_ctl` 将监听socket注册到epoll实例中，监听可读事件。
    - 具体而言，需要创建一个`epoll_event`类型的数组，作为事件列表的返回位置
    - 需要创建一个`epoll_event`类型的变量：
        - 设置其成员 `events` 为 `EPOLLIN`（表示监听可读事件）
        - 为什么要设置这个？
        - 因为我们要告诉Epoll，我们对这个 socket 感兴趣的事件类型是什么
        - 一个 socket 在以下几种情况是活跃的：
            - 可读：有数据可读，或者有新的连接请求（对于监听socket）
            - 可写：发送缓冲区有空间可写
            - 错误：发生错误
            - 关闭：连接被关闭
        - 如果这里我们不设置`EPOLLIN`，Epoll将会因为socket可写而持续触发事件，导致CPU占用率飙升
        - 即使你明确了只关注“可读（EPOLLIN）”，内核依然不知道你喜欢哪种通知方式
            - 水平触发 (Level Triggered, LT) —— 默认模式
            - 只要缓冲区里还有哪怕 1 个字节的数据，内核就会在每一次 epoll_wait 时把你叫醒。
            - 边缘触发 (Edge Triggered, ET) —— 需要额外设置 `EPOLLET` 标志
            - 状态发生突变时，只通知你一次。
            - 突变的定义：新的连接建立，新数据到来，（对于写）发送缓冲区从满变不满。
        - 将监听socket的文件描述符赋值给 `data.fd`
        - 为什么？
        - 如我们所知，Epoll要返回的是一个事件表
        - 那这个事件表中每个元素都是什么，都包含什么？
        - 显然是一个 `epoll_event` 结构体
        - Epoll 不能自己凭空生成事件
        - 它返回的就是我们在这个注册函数中传入的事件变量ev
    - 调用 `epoll_ctl`，传入epoll实例的
        - 文件描述符
        - 操作类型（如 `EPOLL_CTL_ADD` 表示添加）
        - 监听socket的文件描述符
        - 事件变量ev
        - `epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);`
- 服务器进入主循环
- 等待事件：调用 `epoll_wait` 等待事件发生。
    - 这个函数在做什么？
    - 这个函数会阻塞，直到有事件发生
    - 更具体而言，如果没有事件发生，程序会停在这里
    - 然后 OS 会将这个线程设置为可运行状态，当有事件发生时，OS 会将这个线程唤醒
    - 当有事件发生时，`epoll_wait` 会将就绪的事件放入我们之前创建的事件列表中
    - 并返回就绪事件的数量
    - 下一步就要我们来处理这些就绪事件了
- 对于每个就绪事件，检查事件类型并处理相应的事件：
    - 首先，一个 socket 可读，意思就是这个 socket 的接收缓冲区中有数据可读
    - 然后，一个 socket 可写，意思就是这个 socket 的发送缓冲区中有空间可写
    - 现在让我们开始分类处理事件
    - 如果事件是监听socket的可读事件：
        - 说明有新的连接请求到来
        - 我们需要调用 `accept` 来接受这个连接
        - 并将新的连接socket注册到epoll实例中，监听可读事件。
    - 如果事件是连接socket的可读事件：
        - 说明这个连接有数据可读
        - 我们需要调用 `read` 来读取数据
        - 并根据需要进行处理。
    - 如果事件是连接socket的可写事件：
        - 说明这个连接的发送缓冲区有空间可写
        - 我们需要调用 `write` 来发送数据。
    - 如果事件是连接socket的错误事件：
        - 说明这个连接发生了错误
        - 我们需要关闭这个连接，并从epoll实例中删除它。
- 循环继续等待事件，直到服务器关闭。


Epoll的目的：
- 高效的事件通知机制：Epoll使用事件驱动模型，避免了轮询带来的性能问题。
- 支持大量连接：Epoll能够同时处理成千上万的连接，不阻塞。
- 较低的系统调用开销：Epoll通过内核事件表来管理文件描述符，减少了系统调用的次数。

Epoll的工作原理：
- Epoll使用一个内核事件表来管理文件描述符。当一个文件描述符注册到epoll实例中时，内核会将其添加到事件表中。
- 这个表是基于红黑树和链表实现的。：
  - 红黑树用于存储所有注册的文件描述符，支持高效的插入、删除和查找操作。
  - 链表用于存储就绪的文件描述符，支持快速遍历和处理。
- 应用程序通过调用 `epoll_wait` 来等待事件的发生。当有事件发生时，应用程序会收到一个事件列表，其中包含了发生事件的文件描述符和事件类型。
- 应用程序可以根据事件类型来处理相应的事件，例如读取数据、发送数据或关闭连接。


- 为什么我们使用 epoll 的流程如此繁琐？
    - epoll 是一个内核中的机制
    - 如果允许用户态程序直接操作 epoll 的数据结构，可能会导致内核崩溃
    - 同时用户态程序也不应该负责处理 epoll 需要解决的同步问题（如线程安全、事件分发等）
    - 这也是将操作系统分离为内核态和用户态的原因之一
- 这里为了保持代码的清晰和可维护，我们需要封装 epoll 的相关操作
    - 这也是我们引入 Epoll 类的原因
    - 流程化的 epoll 操作代码被我们封装在 Epoll 类的成员函数中
    - 这样我们就可以通过调用这些成员函数来完成 epoll 的相关操作
    - 而不需要直接操作 epoll 的数据结构
    - 也不会做“在 main 函数里放置一个全局变量（事件数组）来接收 epoll_wait 的返回值“这种事情


我会发现有一大堆类型，都看起来像是无符号32位整数：
- `uint32_t`：无符号32位整数，范围从0到4294967295。
- `unsigned int`：无符号整数，通常也是32位，但具体大小取决于平台。
- 为什么会有一大堆？
- 根本原因：历史 + 平台差异 + 标准层叠
- 最原始的C标准只保证：
    - sizeof(short) <= sizeof(int) <= sizeof(long)
    - 但没有规定：
        - int 是不是 32 位？
        - long 是不是 64 位？
        - 在windows上，int 是 32 位，long 也是 32 位
        - 在linux上，int 是 32 位，long 是 64 位
        - 这就导致了平台差异
- 为了应对这种差异，C99引入了 `<stdint.h>` 头文件
- 定义了固定宽度的整数类型，如 `uint32_t` 和 `int64_t`
- 本质上大部分是`typedef unsigned int uint32_t;`这样的定义



目前，我们使用Epoll的方式非常原始，这表现为：
    - 我们只使用 fd 一个整数来代表一个连接
    - 仅仅通过 fd 无法描述完整的连接状态
        - 比如：这个 fd 是监听socket 还是连接socket？
        - 我们应该对它进行 accept 还是 read/write？
    - 这些信息的确定发生在我们将 fd 注册到 Epoll 实例中的时候
        - 例如，我们调用 Epoll 的方法设置EPOLLIN
        - 调用 Epoll 的方法将 fd 注册到 Epoll 实例中
    - 在现在的实现中，这些过程都发生在 main 函数里
    - 换句话说，这些属于 fd 的信息被隐含在了 main 函数里
    - 这不符合面向对象的设计原则，会导致代码的不可维护
    - 因此我们需要引入一个新的类来封装这些信息
解决方案：
- 我们设计一个 Channel 类，把下述信息打包在一起：
    - fd：文件描述符。
    - events：我们要监听这 fd 的什么事件（如 EPOLLIN）。
    - revents：目前实际在这个 fd 上发生了什么事件（received events）。
    - Epoll*：这个 Channel 属于哪个 Epoll 实例管（方便直接通过 Channel 调用 update）。
- 简而言之：Channel = fd + 其相关的事件 + 其属性。
- 这样一来，我们作注册的时候，在 main 函数里表现为：
    - 创建一个 Channel 实例，传入 fd 和事件信息
    - 调用 Channel 的方法将自己注册到 Epoll 实例中
- 同时，我们对 Epoll 的任何操作也都必须通过 Channel 来完成
    - 例如：更新事件信息，删除事件等
    - 我们仅在初始化时直接操作了 Epoll 来创建实例


现在 server.cpp 里还是充满了 while(true) 循环，以及一堆判断 events 的逻辑。
- 这依然是面向过程的写法。
- 我们要把 main 函数里那个 while(true) 循环，以及里面的 epoll_wait 逻辑，全部移到一个叫 EventLoop 的类里面去。
- 这样我们就可以在创建监听socket以后，用一行代码完成：
    - 把监听socket注册到 EventLoop 里
    - 定义好这个监听socket的事件处理逻辑
    - 比如：当有新连接时，调用 accept，然后把新的连接socket注册到 EventLoop 里
    - 这都是服务器socket需要做的配置
- 我们解决这一问题的方式就是：引入事件循环
- 也就是把 main 函数里那个 while(true) 循环，以及里面的 epoll_wait 逻辑，全部移到一个叫 EventLoop 的类里面去


现在我们的服务器逻辑没有变化，但其代码结构变化如下：
- 我们已经让 Channel 类持有了 fd 和事件相关的信息
    - 但是这还不够
    - 我们还需要让 Channel 类持有事件的处理逻辑
    - 也就是说，我们需要让 Channel 类持有一个回调函数（callback）
    - 这个回调函数会在事件发生时被调用
    - 实现方式就是增加一个成员变量：std::function<void()> callback;
- 然后我们将引入事件循环 EventLoop 类
    - 原本的 Channel 类持有了一个 Epoll* 指针，表示这个 Channel 属于哪个 Epoll 实例管
    - 我们原本这样做的意图如前文所述，是为了将无法用 fd 来描述的连接信息封装在 Channel 类里
    - 但是显然，Epoll 是全局性的资源，所有 Channel 都属于同一个 Epoll 实例管
    - 于是我们将 Channel 类中的 Epoll* 指针替换为 EventLoop* 指针
    - 转而让 EventLoop 类来持有 Epoll 实例
    - 同时将循环处理一次“epoll_wait返回的事件列表”的逻辑放到 EventLoop 类里
    - 具体而言，EventLoop 类会有一个成员函数 loop()
    - 在这个函数里会有一个 while(true) 循环
    - 在这个循环里会调用 Epoll 的 wait() 方法来等待事件的发生
    - 当事件发生时，这个函数还会负责遍历事件列表
    - 然后调用每个事件对应的 Channel 的回调函数来处理事件


现在我们成功的将服务器的事件处理逻辑封装在了 Channel 类里，并将事件循环的逻辑封装在了 EventLoop 类里。

但是还有一个问题，我们在创建服务器时，仍然需要：
- 在 main 函数里写一大段代码来创建监听 channel 
    - 这是 Channel 的初始化
    - 是建立一个服务器的必要步骤其一
- 在 main 函数原本的那个 while(true) 循环里写一个判断：
    - 如果事件是监听 channel 的可读事件：
        - 说明有新的连接请求到来
        - 我们需要调用 accept 来接受这个连接
        - 并将新的连接 channel 注册到epoll实例中，监听可读事件。
    - 如果事件是连接 channel 的可读事件：
        - 说明这个连接有数据可读
        - 我们需要调用 read 来读取数据
        - 并根据需要进行处理。
    - 现在看看我们的情况：
        - 我们将连接 channel 的可读事件的处理逻辑封装在了 Channel 类里，作为回调函数存在
        - 这个回调函数的调用又被 EventLoop 的事件循环封装了
        - 换句话说，整个循环中的第二个分支判断已经被我们封装了
        - 显然，我们接下来要做的就是把监听 channel 处理与它相关的事件的逻辑，也接入到 EventLoop 里
- 梳理一下，我们要做的服务器建立需要两步：
    - 创建监听 channel
    - 定义好这个监听 channel 的事件处理逻辑
        - 也就是前面反复说的，当有新连接时，调用 accept，然后把新的连接 channel 注册到 EventLoop 里
        - 当有数据可读时，调用 read 来读取数据，并根据需要进行处理。
- 只要我们建立一个`Server`类来完成这两步，我们就可以在 main 函数里用一行代码来创建服务器了
- 这就是我们引入 Server 类的原因
    - 其有一个成员函数`handleReadEvent()`，描述了监听 channel 的事件处理逻辑
        - 我们通过`bind`函数将它的签名适配为`std::function<void()>`
        - 并将其作为监听channel的回调函数，传入构造函数中
        - 回顾我们已经做的层层封装：
            - 当 Channel 的构造函数被调用
            - 它会通过我们传入的 EventLoop* 指针来建立与 EventLoop 的联系
            - 此时它还只是一个 Channel 实例，尚未注册到 epoll 里
            - 我们随后调用了 Channel 的 enableReading() 方法
            - 这个方法会调用 EventLoop 的 updateChannel() 方法
            - 这个方法又会调用 Epoll 的 updateChannel() 方法
            - 此时 Channel 才真正被注册到 epoll 里
            - 同理，以后 Eventloop 也会在每次循环时通过其持有的 Epoll 实例来调用 Epoll 的 poll() 方法
            - 这个方法会调用 OS 提供的 Epoll 的 epoll_wait() 方法来等待事件的发生
            - 事件发生后，也会逆向顺着以上的调用链引发 Channel 的回调函数调用
    - 其有一个成员变量`EventLoop* loop_`，表示这个服务器属于哪个事件循环
    - 我们将这个 EventLoop* 指针保存在 Server 类中，以便后续可以调用其 loop() 方法来启动事件循环

现在我们可以在 main 函数里用一行代码来创建服务器了：
- 让我们再梳理一遍从的服务器创建开始，到有第一个客户端连上来之间的完整过程：
    - 首先，我们在 main 函数里创建一个 EventLoop 实例，表示我们的事件循环。
    - 然后，我们创建一个 Server 实例，传入 EventLoop 的指针。
        - 在 Server 的构造函数里，我们会创建一个监听 channel，并将其注册到 EventLoop 里
        - 也就是说，他会被注册到 EventLoop 持有的 Epoll 实例里
        - 同时我们还会定义好这个监听 channel 的事件处理逻辑
        -（当有新连接时，调用 accept，然后把新的连接 channel 注册到 EventLoop 里）
        - 这个时候程序还是在简单顺序执行的，没有阻塞在任何地方
    - 接下来，我们调用 EventLoop 的 loop() 方法来启动事件循环。
        - 这个方法会进入一个 while(true) 循环
        - 并在循环里调用 Epoll 的 poll() 方法
        - 程序在此发生阻塞，等待 epoll 的回应导致 OS 将这个线程唤醒
    - 此时，服务器已经开始监听客户端的连接请求了。
    - 当有第一个客户端连上来时，监听 channel 的可读事件就会被触发。
    - 这个事件会被 EventLoop 通过上述调用链捕获
    - 并调用对应的 Channel 的回调函数来处理这个事件。
    - 在这个回调函数里，我们会调用 accept 来接受这个连接，配置它的业务函数，将其作为新的连接 channel 注册到 EventLoop 里。
    - 随后，程序继续留在 EventLoop 的事件循环里，等待下一个事件的发生。
    - 同样的，程序阻塞在 epoll_wait 里，被挂起，等待 OS 的唤醒。

为什么要把 Server 和 EventLoop 分开？
- 这也是面向对象设计的原则之一：单一职责原则
    - Server 的职责是：
        - 监听端口
        - accept 新连接
        - 创建连接对象
        - 设置回调
        - 维护连接集合
        - 关闭连接
    - EventLoop 的职责是：
        - epoll_wait
        - 维护 fd 列表
        - 触发回调
        - 它不应该知道：
            - 业务协议
            - 连接生命周期策略
            - 线程池
        - 它是纯粹的 IO 调度器。
- 分开的真正主要原因：
    - 这样我们就可以在不同的服务器实现中复用同一个 EventLoop 类了
    - 例如，我们可以实现一个 HTTP 服务器，一个 FTP 服务器，甚至一个 WebSocket 服务器
    - 这些服务器的业务逻辑完全不同，但它们都可以使用同一个 EventLoop 来处理 IO 事件
    - 我们可以：
        - 一个 Server 用一个 EventLoop
        - 多个 Server 共用一个 EventLoop
        - 一个线程一个 EventLoop（多 Reactor 模式，这个后面再解释）

我们分离了服务器类和事件驱动类，将服务器逐渐开发成Reactor模式。
至此，所有服务器逻辑（目前只有接受新连接和echo客户端发来的数据）都写在`Server`类里。
但很显然，`Server`作为一个服务器类，应该更抽象、更通用。
Day 6 的代码虽然能跑，但 Server 类承担了太多的责任：
- 既要负责 "监听连接"（Bind/Listen/Accept）
- 又要负责 "处理新连接的业务"（Read/Write）。
我们要把 "只负责把新连接接进来的组件" 剥离出来，这就是 Acceptor。
- Acceptor：专注于 Listen Socket。
    - 它持有监听套接字，关注连接事件。
    - 当有连接进来时，它 accept 得到 fd，然后把它扔给 Server。
- Server：专注于业务调度。
    - 它不再管 bind/listen 这些底层细节
    - 只等待 Acceptor 给它塞一个新的连接 fd，然后处理后续逻辑。

代码现在逻辑已经非常清晰了：Acceptor 管进门，Server 管调度。

但是，我们一直搁置了一个巨大的隐患：生命周期管理。
到现在为止，我们还在 Server 的回调里写着：
    - new Socket
    - new Channel
    - 在用临时的 lambda 表达式处理读写
- 一旦连接断开，谁来负责删除 Socket, Channel, InetAddress？
- 现在的代码里充满了 delete 的坑，或者干脆就是内存泄漏。


引入 Connection 类。
- 这是 TCP 连接的终极封装。
    - 它持有：
        - Socket
        - Channel
        - EventLoop。
    - 它负责：
        - 处理这条连接的所有读写事件（不再用临时 lambda）。
    - 它管理：
        - 资源的销毁。
        - 当连接断开时，Connection 析构，顺带把 socket 和 channel 带走。
Server 将不再直接持有 socket，而是持有 std::map<int, Connection*>。


修改好后在 Day 8 的代码中，我们手动管理 new 和 delete：
- Acceptor new 出来的 Socket 交给 Connection 持有。
- Connection new 出来交给 Server 的 map 持有。
- 客户端断开导致以下调用链：
    - 触发 Connection::echoRead
    - 调用回调 Server::deleteConnection
    - 从 map 移除并 delete Connection
    - Connection 析构
    - delete Socket。
这一条链非常清晰，是 C++ 项目中经典的资源管理模式。
在更现代的 C++ 中，这里通常会用 std::shared_ptr 和 std::enable_shared_from_this 来自动管理



到目前为止，数据读写是非常“暴力”的：
```
char buf[1024];
read(fd, buf, ...);
write(fd, buf, ...);
```
这有两个严重问题：
- 应用层粘包/半包（Packet Sticking/Splitting）：
    - TCP 是字节流协议，没有“包”的概念。
    - 你发了 "Hello"，又发了 "World"
    - 服务器可能会收到 "HelloWor"，也可能收到 "He" 和 "lloWorld"
    - 协议栈不承诺你每次 read 就能读到一个完整的逻辑包
    - 它只保证顺序传输和数据完整性
    - 单纯的 read(buf) 无法处理这种情况
    - 我们需要一个缓冲区先把数据攒着，凑够一个完整的逻辑包再处理。
- 写阻塞：
    - 如果发送大量数据，write 缓冲区满了
    - 也就是协议栈内部的发送缓冲区满了（这个过程我们无法控制）
    - 调用 write 会阻塞（或者返回 EAGAIN）。
    - 在 Reactor 模式下，绝对不能阻塞。
    - 因为如果阻塞了，整个事件循环就停了，其他连接的事件也无法处理了。
    - 如果一次写不完，我们需要把剩下的数据存起来，等 socket 可写了（EPOLLOUT）再接着写。
比 Day 8，Day 9 引入 Buffer 类来解决这两个问题：
- Day 8 (无 Buffer)
    - 读数据	直接 read 到栈上 char buf[1024]。
    - 数据边界	读到多少算多少，可能只读到 "Hel"。没地方存，必须马上处理。
    - 写数据	直接 write。如果内核缓冲区满了，剩下的数据直接丢弃（或阻塞线程）。
    - 写完成	发送完就完了，不知道什么时候还可以继续写。
- Day 9 (有 Buffer)
    - 先 readv 到栈，然后 append 进 inputBuffer_。
    - 数据都攒在 inputBuffer_ 里，业务逻辑可以只有在攒够一个完整包（比如等到 \n）时才处理。
    - 先试着 write。如果没写完，剩下的存进 outputBuffer_，并自动关注 EPOLLOUT 事件。
    - 当 socket 变为空闲（触发 EPOLLOUT），自动继续发送 Buffer 里剩余的数据。

数据流追踪：从“接收”到“发送”
假设客户端发来 "Hello World"。

1. 读流程 (Incoming)
    - 触发点：内核通知 Epoll：“Hey，fd 5 有数据来了 (EPOLLIN)”。
    - EventLoop::loop() 被 epoll_wait 唤醒。
    - EventLoop 找到对应的 Channel，调用 Channel::handleEvent()。
    - Channel 发现是读事件，调用 readCallback。
        - Day 8：回调是 Connection::echoRead。
            - 直接 read(fd, buf, 1024)。
            - 直接 write(fd, buf) 回显。
        - Day 9：回调是 Connection::handleRead。
            - 调用 inputBuffer_.readFd(fd)。
            - 这里有一个小优化：它准备了 64KB 的栈空间，利用 readv 一次性尽可能多地读入数据。
            - 如果数据很少（"Hello"），直接进了 vector<char>。
            - 如果数据很多，溢出到栈上，然后 append 进 vector（自动扩容）。
    - 数据现在安稳地躺在 inputBuffer_ 里了。
    - Connection 从 inputBuffer_ 里取出字符串，打印出来。
2. 写流程 (Outgoing)
    - 触发点：业务逻辑决定要发数据回给客户端。
        - Day 8：直接调用 write(fd, "Hello", 5)。
            - 隐患：假设这时候网卡此时堵住了（TCP 发送窗口为 0）
            - write 返回 -1 (EAGAIN) 或者只发了 2 个字节。
            - 剩下的 "llo" 怎么办？
            - Day 8 的代码直接忽略了，这就叫丢包（应用层丢包）。
        - Day 9：调用 Connection::send("Hello")。
            - Try Write: 如果 outputBuffer_ 是空的，说明之前没积压，直接尝试 write。
            - Buffer Append: 假设只写出去了 "He" (2 bytes)，还剩 "llo"。
            - 代码把 "llo" (3 bytes) 追加到 outputBuffer_。
            - Enable EPOLLOUT: 关键一步！调用 channel_->enableWriting()。
            - 这就相当于告诉 Epoll：“这个 fd，一旦网卡不堵了（可写了），就是活跃状态，应当被唤醒”
3. 追写流程 (Post-Write)
    - 触发点：过了一会儿，网卡终于疏通了，内核缓冲区有了空位。
        - Epoll 通知：fd 5 可写 (EPOLLOUT)。
        - EventLoop -> Channel::handleEvent -> Channel::writeCallback。
    - Day 9 新增：回调是 Connection::handleWrite。
    - 这里的逻辑很简单：只要 outputBuffer_ 还有数据，就继续 write。
    - 这次成功把剩下的 "llo" 发出去了。
    - outputBuffer_ 空了。
    - Disable EPOLLOUT: 任务完成，调用 channel_->disableWriting()。
    - 告诉 Epoll：“当前 fd 若协议栈缓冲区空闲，不算活跃状态”。
    - 如果不取消，Epoll 会疯狂通过 busy-loop 通知你“可写可写可写”，CPU 会飙升到 100%。


现在进入 Day 10：线程池（Thread Pool）。
Day 10 的目标：解放主 Reactor
    - 目前的架构是 单线程 Reactor 模式：
    - Acceptor 监听连接。
    - Connection 读数据。
    - 业务逻辑（比如计算 1+1）。
    - Connection 写数据。
    - 所有这一切都在同一个 main 线程里完成。
解决方案：
- 引入线程池。
    - 主线程（Reactor）只负责“收发数据”（IO 密集型）
    - 复杂的业务逻辑扔给工作线程池（CPU 密集型）去跑。