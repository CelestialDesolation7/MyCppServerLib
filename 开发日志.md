# MyCppServerLib 开发日志

服务器的生命周期如下：


- socket: 创建套接字。
    - 这是一个整数句柄，指向一个内核中的结构体
- bind: 绑定 IP 和端口（这相当于实例化了这个 socket 的存在本身）。
- listen: 监听（这会让 OS 将这个 socket 标记为监听类 socket）。
- accept: 接收连接（这是一个阻塞操作，程序会停在这里直到有客户端连上来）。
- read/write: 通信


socket 分为两种类型：
- 监听socket：用于监听客户端连接的套接字。（永远不会有目的端口，只是为了处理建立新连接的请求）
    - 它具有两个队列：
        - 已完成连接队列：存储已经完成三次握手的连接，等待服务器调用 `accept` 来处理。
        - 半连接队列：存储正在进行三次握手的连接，等待完成后进入已完成连接队列。
        - 握手过程是 OS 的责任，服务器程序不需要关心
- 连接socket：用于与客户端通信的套接字。（含有完整的五元组：源IP、源端口、目的IP、目的端口、协议）
    - 它具有发送和接受缓冲区。


IO复用：一个线程（服务器线程，复用者）同时监控多个
socket_fd（文件描述符，对应于多个IO事件）


Epoll的生命周期如下：
- 创建epoll实例：调用 `epoll_create` 创建一个epoll实例。
- 注册文件描述符：使用 `epoll_ctl` 将监听socket注册到epoll实例中，监听可读事件。
    - 具体而言，需要创建一个`epoll_event`类型的数组，作为事件列表的返回位置
    - 需要创建一个`epoll_event`类型的变量：
        - 设置其成员 `events` 为 `EPOLLIN`（表示监听可读事件）
        - 为什么要设置这个？
        - 因为我们要告诉Epoll，我们对这个 socket 感兴趣的事件类型是什么
        - 一个 socket 在以下几种情况是活跃的：
            - 可读：有数据可读，或者有新的连接请求（对于监听socket）
            - 可写：发送缓冲区有空间可写
            - 错误：发生错误
            - 关闭：连接被关闭
        - 如果这里我们不设置`EPOLLIN`，Epoll将会因为socket可写而持续触发事件，导致CPU占用率飙升
        - 即使你明确了只关注“可读（EPOLLIN）”，内核依然不知道你喜欢哪种通知方式
            - 水平触发 (Level Triggered, LT) —— 默认模式
            - 只要缓冲区里还有哪怕 1 个字节的数据，内核就会在每一次 epoll_wait 时把你叫醒。
            - 边缘触发 (Edge Triggered, ET) —— 需要额外设置 `EPOLLET` 标志
            - 状态发生突变时，只通知你一次。
            - 突变的定义：新的连接建立，新数据到来，（对于写）发送缓冲区从满变不满。
        - 将监听socket的文件描述符赋值给 `data.fd`
        - 为什么？
        - 如我们所知，Epoll要返回的是一个事件表
        - 那这个事件表中每个元素都是什么，都包含什么？
        - 显然是一个 `epoll_event` 结构体
        - Epoll 不能自己凭空生成事件
        - 它返回的就是我们在这个注册函数中传入的事件变量ev
    - 调用 `epoll_ctl`，传入epoll实例的
        - 文件描述符
        - 操作类型（如 `EPOLL_CTL_ADD` 表示添加）
        - 监听socket的文件描述符
        - 事件变量ev
        - `epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);`
- 服务器进入主循环
- 等待事件：调用 `epoll_wait` 等待事件发生。
    - 这个函数在做什么？
    - 这个函数会阻塞，直到有事件发生
    - 更具体而言，如果没有事件发生，程序会停在这里
    - 然后 OS 会将这个线程设置为可运行状态，当有事件发生时，OS 会将这个线程唤醒
    - 当有事件发生时，`epoll_wait` 会将就绪的事件放入我们之前创建的事件列表中
    - 并返回就绪事件的数量
    - 下一步就要我们来处理这些就绪事件了
- 对于每个就绪事件，检查事件类型并处理相应的事件：
    - 首先，一个 socket 可读，意思就是这个 socket 的接收缓冲区中有数据可读
    - 然后，一个 socket 可写，意思就是这个 socket 的发送缓冲区中有空间可写
    - 现在让我们开始分类处理事件
    - 如果事件是监听socket的可读事件：
        - 说明有新的连接请求到来
        - 我们需要调用 `accept` 来接受这个连接
        - 并将新的连接socket注册到epoll实例中，监听可读事件。
    - 如果事件是连接socket的可读事件：
        - 说明这个连接有数据可读
        - 我们需要调用 `read` 来读取数据
        - 并根据需要进行处理。
    - 如果事件是连接socket的可写事件：
        - 说明这个连接的发送缓冲区有空间可写
        - 我们需要调用 `write` 来发送数据。
    - 如果事件是连接socket的错误事件：
        - 说明这个连接发生了错误
        - 我们需要关闭这个连接，并从epoll实例中删除它。
- 循环继续等待事件，直到服务器关闭。


Epoll的目的：
- 高效的事件通知机制：Epoll使用事件驱动模型，避免了轮询带来的性能问题。
- 支持大量连接：Epoll能够同时处理成千上万的连接，不阻塞。
- 较低的系统调用开销：Epoll通过内核事件表来管理文件描述符，减少了系统调用的次数。

Epoll的工作原理：
- Epoll使用一个内核事件表来管理文件描述符。当一个文件描述符注册到epoll实例中时，内核会将其添加到事件表中。
- 这个表是基于红黑树和链表实现的。：
  - 红黑树用于存储所有注册的文件描述符，支持高效的插入、删除和查找操作。
  - 链表用于存储就绪的文件描述符，支持快速遍历和处理。
- 应用程序通过调用 `epoll_wait` 来等待事件的发生。当有事件发生时，应用程序会收到一个事件列表，其中包含了发生事件的文件描述符和事件类型。
- 应用程序可以根据事件类型来处理相应的事件，例如读取数据、发送数据或关闭连接。


- 为什么我们使用 epoll 的流程如此繁琐？
    - epoll 是一个内核中的机制
    - 如果允许用户态程序直接操作 epoll 的数据结构，可能会导致内核崩溃
    - 同时用户态程序也不应该负责处理 epoll 需要解决的同步问题（如线程安全、事件分发等）
    - 这也是将操作系统分离为内核态和用户态的原因之一
- 这里为了保持代码的清晰和可维护，我们需要封装 epoll 的相关操作
    - 这也是我们引入 Epoll 类的原因
    - 流程化的 epoll 操作代码被我们封装在 Epoll 类的成员函数中
    - 这样我们就可以通过调用这些成员函数来完成 epoll 的相关操作
    - 而不需要直接操作 epoll 的数据结构
    - 也不会做“在 main 函数里放置一个全局变量（事件数组）来接收 epoll_wait 的返回值“这种事情


我会发现有一大堆类型，都看起来像是无符号32位整数：
- `uint32_t`：无符号32位整数，范围从0到4294967295。
- `unsigned int`：无符号整数，通常也是32位，但具体大小取决于平台。
- 为什么会有一大堆？
- 根本原因：历史 + 平台差异 + 标准层叠
- 最原始的C标准只保证：
    - sizeof(short) <= sizeof(int) <= sizeof(long)
    - 但没有规定：
        - int 是不是 32 位？
        - long 是不是 64 位？
        - 在windows上，int 是 32 位，long 也是 32 位
        - 在linux上，int 是 32 位，long 是 64 位
        - 这就导致了平台差异
- 为了应对这种差异，C99引入了 `<stdint.h>` 头文件
- 定义了固定宽度的整数类型，如 `uint32_t` 和 `int64_t`
- 本质上大部分是`typedef unsigned int uint32_t;`这样的定义



目前，我们使用Epoll的方式非常原始，这表现为：
    - 我们只使用 fd 一个整数来代表一个连接
    - 仅仅通过 fd 无法描述完整的连接状态
        - 比如：这个 fd 是监听socket 还是连接socket？
        - 我们应该对它进行 accept 还是 read/write？
    - 这些信息的确定发生在我们将 fd 注册到 Epoll 实例中的时候
        - 例如，我们调用 Epoll 的方法设置EPOLLIN
        - 调用 Epoll 的方法将 fd 注册到 Epoll 实例中
    - 在现在的实现中，这些过程都发生在 main 函数里
    - 换句话说，这些属于 fd 的信息被隐含在了 main 函数里
    - 这不符合面向对象的设计原则，会导致代码的不可维护
    - 因此我们需要引入一个新的类来封装这些信息
解决方案：
- 我们设计一个 Channel 类，把下述信息打包在一起：
    - fd：文件描述符。
    - events：我们要监听这 fd 的什么事件（如 EPOLLIN）。
    - revents：目前实际在这个 fd 上发生了什么事件（received events）。
    - Epoll*：这个 Channel 属于哪个 Epoll 实例管（方便直接通过 Channel 调用 update）。
- 简而言之：Channel = fd + 其相关的事件 + 其属性。
- 这样一来，我们作注册的时候，在 main 函数里表现为：
    - 创建一个 Channel 实例，传入 fd 和事件信息
    - 调用 Channel 的方法将自己注册到 Epoll 实例中
- 同时，我们对 Epoll 的任何操作也都必须通过 Channel 来完成
    - 例如：更新事件信息，删除事件等
    - 我们仅在初始化时直接操作了 Epoll 来创建实例


现在 server.cpp 里还是充满了 while(true) 循环，以及一堆判断 events 的逻辑。
- 这依然是面向过程的写法。
- 我们要把 main 函数里那个 while(true) 循环，以及里面的 epoll_wait 逻辑，全部移到一个叫 EventLoop 的类里面去。
- 这样我们就可以在创建监听socket以后，用一行代码完成：
    - 把监听socket注册到 EventLoop 里
    - 定义好这个监听socket的事件处理逻辑
    - 比如：当有新连接时，调用 accept，然后把新的连接socket注册到 EventLoop 里
    - 这都是服务器socket需要做的配置
- 我们解决这一问题的方式就是：引入事件循环
- 也就是把 main 函数里那个 while(true) 循环，以及里面的 epoll_wait 逻辑，全部移到一个叫 EventLoop 的类里面去


现在我们的服务器逻辑没有变化，但其代码结构变化如下：
- 我们已经让 Channel 类持有了 fd 和事件相关的信息
    - 但是这还不够
    - 我们还需要让 Channel 类持有事件的处理逻辑
    - 也就是说，我们需要让 Channel 类持有一个回调函数（callback）
    - 这个回调函数会在事件发生时被调用
    - 实现方式就是增加一个成员变量：std::function<void()> callback;
- 然后我们将引入事件循环 EventLoop 类
    - 原本的 Channel 类持有了一个 Epoll* 指针，表示这个 Channel 属于哪个 Epoll 实例管
    - 我们原本这样做的意图如前文所述，是为了将无法用 fd 来描述的连接信息封装在 Channel 类里
    - 但是显然，Epoll 是全局性的资源，所有 Channel 都属于同一个 Epoll 实例管
    - 于是我们将 Channel 类中的 Epoll* 指针替换为 EventLoop* 指针
    - 转而让 EventLoop 类来持有 Epoll 实例
    - 同时将循环处理一次“epoll_wait返回的事件列表”的逻辑放到 EventLoop 类里
    - 具体而言，EventLoop 类会有一个成员函数 loop()
    - 在这个函数里会有一个 while(true) 循环
    - 在这个循环里会调用 Epoll 的 wait() 方法来等待事件的发生
    - 当事件发生时，这个函数还会负责遍历事件列表
    - 然后调用每个事件对应的 Channel 的回调函数来处理事件


现在我们成功的将服务器的事件处理逻辑封装在了 Channel 类里，并将事件循环的逻辑封装在了 EventLoop 类里。

但是还有一个问题，我们在创建服务器时，仍然需要：
- 在 main 函数里写一大段代码来创建监听 channel 
    - 这是 Channel 的初始化
    - 是建立一个服务器的必要步骤其一
- 在 main 函数原本的那个 while(true) 循环里写一个判断：
    - 如果事件是监听 channel 的可读事件：
        - 说明有新的连接请求到来
        - 我们需要调用 accept 来接受这个连接
        - 并将新的连接 channel 注册到epoll实例中，监听可读事件。
    - 如果事件是连接 channel 的可读事件：
        - 说明这个连接有数据可读
        - 我们需要调用 read 来读取数据
        - 并根据需要进行处理。
    - 现在看看我们的情况：
        - 我们将连接 channel 的可读事件的处理逻辑封装在了 Channel 类里，作为回调函数存在
        - 这个回调函数的调用又被 EventLoop 的事件循环封装了
        - 换句话说，整个循环中的第二个分支判断已经被我们封装了
        - 显然，我们接下来要做的就是把监听 channel 处理与它相关的事件的逻辑，也接入到 EventLoop 里
- 梳理一下，我们要做的服务器建立需要两步：
    - 创建监听 channel
    - 定义好这个监听 channel 的事件处理逻辑
        - 也就是前面反复说的，当有新连接时，调用 accept，然后把新的连接 channel 注册到 EventLoop 里
        - 当有数据可读时，调用 read 来读取数据，并根据需要进行处理。
- 只要我们建立一个`Server`类来完成这两步，我们就可以在 main 函数里用一行代码来创建服务器了
- 这就是我们引入 Server 类的原因
    - 其有一个成员函数`handleReadEvent()`，描述了监听 channel 的事件处理逻辑
        - 我们通过`bind`函数将它的签名适配为`std::function<void()>`
        - 并将其作为监听channel的回调函数，传入构造函数中
        - 回顾我们已经做的层层封装：
            - 当 Channel 的构造函数被调用
            - 它会通过我们传入的 EventLoop* 指针来建立与 EventLoop 的联系
            - 此时它还只是一个 Channel 实例，尚未注册到 epoll 里
            - 我们随后调用了 Channel 的 enableReading() 方法
            - 这个方法会调用 EventLoop 的 updateChannel() 方法
            - 这个方法又会调用 Epoll 的 updateChannel() 方法
            - 此时 Channel 才真正被注册到 epoll 里
            - 同理，以后 Eventloop 也会在每次循环时通过其持有的 Epoll 实例来调用 Epoll 的 poll() 方法
            - 这个方法会调用 OS 提供的 Epoll 的 epoll_wait() 方法来等待事件的发生
            - 事件发生后，也会逆向顺着以上的调用链引发 Channel 的回调函数调用
    - 其有一个成员变量`EventLoop* loop_`，表示这个服务器属于哪个事件循环
    - 我们将这个 EventLoop* 指针保存在 Server 类中，以便后续可以调用其 loop() 方法来启动事件循环

现在我们可以在 main 函数里用一行代码来创建服务器了：
- 让我们再梳理一遍从的服务器创建开始，到有第一个客户端连上来之间的完整过程：
    - 首先，我们在 main 函数里创建一个 EventLoop 实例，表示我们的事件循环。
    - 然后，我们创建一个 Server 实例，传入 EventLoop 的指针。
        - 在 Server 的构造函数里，我们会创建一个监听 channel，并将其注册到 EventLoop 里
        - 也就是说，他会被注册到 EventLoop 持有的 Epoll 实例里
        - 同时我们还会定义好这个监听 channel 的事件处理逻辑
        -（当有新连接时，调用 accept，然后把新的连接 channel 注册到 EventLoop 里）
        - 这个时候程序还是在简单顺序执行的，没有阻塞在任何地方
    - 接下来，我们调用 EventLoop 的 loop() 方法来启动事件循环。
        - 这个方法会进入一个 while(true) 循环
        - 并在循环里调用 Epoll 的 poll() 方法
        - 程序在此发生阻塞，等待 epoll 的回应导致 OS 将这个线程唤醒
    - 此时，服务器已经开始监听客户端的连接请求了。
    - 当有第一个客户端连上来时，监听 channel 的可读事件就会被触发。
    - 这个事件会被 EventLoop 通过上述调用链捕获
    - 并调用对应的 Channel 的回调函数来处理这个事件。
    - 在这个回调函数里，我们会调用 accept 来接受这个连接，配置它的业务函数，将其作为新的连接 channel 注册到 EventLoop 里。
    - 随后，程序继续留在 EventLoop 的事件循环里，等待下一个事件的发生。
    - 同样的，程序阻塞在 epoll_wait 里，被挂起，等待 OS 的唤醒。

为什么要把 Server 和 EventLoop 分开？
- 这也是面向对象设计的原则之一：单一职责原则
    - Server 的职责是：
        - 监听端口
        - accept 新连接
        - 创建连接对象
        - 设置回调
        - 维护连接集合
        - 关闭连接
    - EventLoop 的职责是：
        - epoll_wait
        - 维护 fd 列表
        - 触发回调
        - 它不应该知道：
            - 业务协议
            - 连接生命周期策略
            - 线程池
        - 它是纯粹的 IO 调度器。
- 分开的真正主要原因：
    - 这样我们就可以在不同的服务器实现中复用同一个 EventLoop 类了
    - 例如，我们可以实现一个 HTTP 服务器，一个 FTP 服务器，甚至一个 WebSocket 服务器
    - 这些服务器的业务逻辑完全不同，但它们都可以使用同一个 EventLoop 来处理 IO 事件
    - 我们可以：
        - 一个 Server 用一个 EventLoop
        - 多个 Server 共用一个 EventLoop
        - 一个线程一个 EventLoop（多 Reactor 模式，这个后面再解释）

我们分离了服务器类和事件驱动类，将服务器逐渐开发成Reactor模式。
至此，所有服务器逻辑（目前只有接受新连接和echo客户端发来的数据）都写在`Server`类里。
但很显然，`Server`作为一个服务器类，应该更抽象、更通用。
Day 6 的代码虽然能跑，但 Server 类承担了太多的责任：
- 既要负责 "监听连接"（Bind/Listen/Accept）
- 又要负责 "处理新连接的业务"（Read/Write）。
我们要把 "只负责把新连接接进来的组件" 剥离出来，这就是 Acceptor。
- Acceptor：专注于 Listen Socket。
    - 它持有监听套接字，关注连接事件。
    - 当有连接进来时，它 accept 得到 fd，然后把它扔给 Server。
- Server：专注于业务调度。
    - 它不再管 bind/listen 这些底层细节
    - 只等待 Acceptor 给它塞一个新的连接 fd，然后处理后续逻辑。

代码现在逻辑已经非常清晰了：Acceptor 管进门，Server 管调度。

但是，我们一直搁置了一个巨大的隐患：生命周期管理。
到现在为止，我们还在 Server 的回调里写着：
    - new Socket
    - new Channel
    - 在用临时的 lambda 表达式处理读写
- 一旦连接断开，谁来负责删除 Socket, Channel, InetAddress？
- 现在的代码里充满了 delete 的坑，或者干脆就是内存泄漏。


引入 Connection 类。
- 这是 TCP 连接的终极封装。
    - 它持有：
        - Socket
        - Channel
        - EventLoop。
    - 它负责：
        - 处理这条连接的所有读写事件（不再用临时 lambda）。
    - 它管理：
        - 资源的销毁。
        - 当连接断开时，Connection 析构，顺带把 socket 和 channel 带走。
Server 将不再直接持有 socket，而是持有 std::map<int, Connection*>。


修改好后在 Day 8 的代码中，我们手动管理 new 和 delete：
- Acceptor new 出来的 Socket 交给 Connection 持有。
- Connection new 出来交给 Server 的 map 持有。
- 客户端断开导致以下调用链：
    - 触发 Connection::echoRead
    - 调用回调 Server::deleteConnection
    - 从 map 移除并 delete Connection
    - Connection 析构
    - delete Socket。
这一条链非常清晰，是 C++ 项目中经典的资源管理模式。
在更现代的 C++ 中，这里通常会用 std::shared_ptr 和 std::enable_shared_from_this 来自动管理



到目前为止，数据读写是非常“暴力”的：
```
char buf[1024];
read(fd, buf, ...);
write(fd, buf, ...);
```
这有两个严重问题：
- 应用层粘包/半包（Packet Sticking/Splitting）：
    - TCP 是字节流协议，没有“包”的概念。
    - 你发了 "Hello"，又发了 "World"
    - 服务器可能会收到 "HelloWor"，也可能收到 "He" 和 "lloWorld"
    - 协议栈不承诺你每次 read 就能读到一个完整的逻辑包
    - 它只保证顺序传输和数据完整性
    - 单纯的 read(buf) 无法处理这种情况
    - 我们需要一个缓冲区先把数据攒着，凑够一个完整的逻辑包再处理。
- 写阻塞：
    - 如果发送大量数据，write 缓冲区满了
    - 也就是协议栈内部的发送缓冲区满了（这个过程我们无法控制）
    - 调用 write 会阻塞（或者返回 EAGAIN）。
    - 在 Reactor 模式下，绝对不能阻塞。
    - 因为如果阻塞了，整个事件循环就停了，其他连接的事件也无法处理了。
    - 如果一次写不完，我们需要把剩下的数据存起来，等 socket 可写了（EPOLLOUT）再接着写。
比 Day 8，Day 9 引入 Buffer 类来解决这两个问题：
- Day 8 (无 Buffer)
    - 读数据	直接 read 到栈上 char buf[1024]。
    - 数据边界	读到多少算多少，可能只读到 "Hel"。没地方存，必须马上处理。
    - 写数据	直接 write。如果内核缓冲区满了，剩下的数据直接丢弃（或阻塞线程）。
    - 写完成	发送完就完了，不知道什么时候还可以继续写。
- Day 9 (有 Buffer)
    - 先 readv 到栈，然后 append 进 inputBuffer_。
    - 数据都攒在 inputBuffer_ 里，业务逻辑可以只有在攒够一个完整包（比如等到 \n）时才处理。
    - 先试着 write。如果没写完，剩下的存进 outputBuffer_，并自动关注 EPOLLOUT 事件。
    - 当 socket 变为空闲（触发 EPOLLOUT），自动继续发送 Buffer 里剩余的数据。

数据流追踪：从“接收”到“发送”
假设客户端发来 "Hello World"。

1. 读流程 (Incoming)
    - 触发点：内核通知 Epoll：“Hey，fd 5 有数据来了 (EPOLLIN)”。
    - EventLoop::loop() 被 epoll_wait 唤醒。
    - EventLoop 找到对应的 Channel，调用 Channel::handleEvent()。
    - Channel 发现是读事件，调用 readCallback。
        - Day 8：回调是 Connection::echoRead。
            - 直接 read(fd, buf, 1024)。
            - 直接 write(fd, buf) 回显。
        - Day 9：回调是 Connection::handleRead。
            - 调用 inputBuffer_.readFd(fd)。
            - 这里有一个小优化：它准备了 64KB 的栈空间，利用 readv 一次性尽可能多地读入数据。
            - 如果数据很少（"Hello"），直接进了 vector<char>。
            - 如果数据很多，溢出到栈上，然后 append 进 vector（自动扩容）。
    - 数据现在安稳地躺在 inputBuffer_ 里了。
    - Connection 从 inputBuffer_ 里取出字符串，打印出来。
2. 写流程 (Outgoing)
    - 触发点：业务逻辑决定要发数据回给客户端。
        - Day 8：直接调用 write(fd, "Hello", 5)。
            - 隐患：假设这时候网卡此时堵住了（TCP 发送窗口为 0）
            - write 返回 -1 (EAGAIN) 或者只发了 2 个字节。
            - 剩下的 "llo" 怎么办？
            - Day 8 的代码直接忽略了，这就叫丢包（应用层丢包）。
        - Day 9：调用 Connection::send("Hello")。
            - Try Write: 如果 outputBuffer_ 是空的，说明之前没积压，直接尝试 write。
            - Buffer Append: 假设只写出去了 "He" (2 bytes)，还剩 "llo"。
            - 代码把 "llo" (3 bytes) 追加到 outputBuffer_。
            - Enable EPOLLOUT: 关键一步！调用 channel_->enableWriting()。
            - 这就相当于告诉 Epoll：“这个 fd，一旦网卡不堵了（可写了），就是活跃状态，应当被唤醒”
3. 追写流程 (Post-Write)
    - 触发点：过了一会儿，网卡终于疏通了，内核缓冲区有了空位。
        - Epoll 通知：fd 5 可写 (EPOLLOUT)。
        - EventLoop -> Channel::handleEvent -> Channel::writeCallback。
    - Day 9 新增：回调是 Connection::handleWrite。
    - 这里的逻辑很简单：只要 outputBuffer_ 还有数据，就继续 write。
    - 这次成功把剩下的 "llo" 发出去了。
    - outputBuffer_ 空了。
    - Disable EPOLLOUT: 任务完成，调用 channel_->disableWriting()。
    - 告诉 Epoll：“当前 fd 若协议栈缓冲区空闲，不算活跃状态”。
    - 如果不取消，Epoll 会疯狂通过 busy-loop 通知你“可写可写可写”，CPU 会飙升到 100%。


现在进入 Day 10：线程池（Thread Pool）。
Day 10 的目标：解放主 Reactor
    - 目前的架构是 单线程 Reactor 模式：
    - Acceptor 监听连接。
    - Connection 读数据。
    - 业务逻辑（比如计算 1+1）。
    - Connection 写数据。
    - 所有这一切都在同一个 main 线程里完成。
解决方案：
- 引入线程池。
    - 主线程（Reactor）只负责“收发数据”（IO 密集型）
    - 复杂的业务逻辑扔给工作线程池（CPU 密集型）去跑。



当前全流程调用链梳理 (Day 9 状态)
假设服务器已编译运行，客户端发起连接并发送 "Hello"。

1. 启动服务器 (Startup)
  - [Main]: 执行 new EventLoop() -> new Server(loop)。
  - [Server] 构造过程:
      - 执行 new Acceptor(loop)：
          - 创建监听 Socket (依次执行申请 socket fd -> bind 地址 -> 设置 listen)。
          - 基于监听 fd 创建 Channel(loop, listenFd)。
          - 设置读回调: setReadCallback(Acceptor::acceptConnection)。
          - 设置写回调: setWriteCallback(nullptr) (注: Acceptor 不会收到写事件)。
          - 开启监听读事件: enableReading()
              - 设置 Channel 事件类型为 EPOLLIN | EPOLLET。
              - 调用 loop->updateChannel()。
              - [EventLoop] 响应并调用 ep->updateChannel()。
              - [Epoll] 将配置注册到 epoll 实例中，即调用 epoll_ctl(EPOLL_CTL_ADD, listenFd)。
      - 设置新连接回调: Acceptor::setNewConnectionCallback(Server::newConnection) (注: 此处为解耦出的回调逻辑)。
  - [Main]: 执行 loop->loop()。
      - 进入 while(!quit) 死循环。
      - 线程阻塞在 epoll_wait，等待事件发生。

2. 收到连接请求 (New Connection)
  - [Kernel]: 三次握手完成，listenFd 变为可读状态。
  - [EventLoop]: epoll_wait 返回，activeChannels 数组被填充。
  - [EventLoop]: 遍历 activeChannels (此时仅有 listenChannel)。
      - 调用 listenChannel->handleEvent()。
      - [Channel]: 触发读事件，调用 readCallback (即 Acceptor::acceptConnection)。
          - [Acceptor] acceptConnection 执行:
              - 调用 socket->accept() 得到 clientFd。
              - 准备建立新 Channel 所需的数据 (如 new Socket(clientFd))。
              - 调用 newConnectionCallback (即 Server::newConnection)。
                  - [Server] newConnection 执行:
                      - 实例化 new Connection(loop, clientFd)。
                      - [Connection] 构造过程:
                          - 创建 Channel(loop, clientFd)。
                          - 设置读回调: setReadCallback(Connection::handleRead)。
                          - 设置写回调: setWriteCallback(Connection::handleWrite)。
                            (注: 目前业务逻辑尚未分离，被硬编码在上述回调函数中)
                          - 连接建立，开始等待读事件: enableReading()
                              - 设置 Channel 事件类型为 EPOLLIN | EPOLLET。
                              - 调用 loop->updateChannel() -> ep->updateChannel()。
                              - [Epoll] 执行 epoll_ctl(EPOLL_CTL_ADD, clientFd)。
                      - [Connection] 构造完成，返回 Connection*。
                  - [Server]: 将返回的新连接加入内部维护的 std::map<int, Connection*> connections。

3. 收到客户端请求 (Data Arrives)
  - [Kernel]: 客户端发来数据 "Hello"，clientFd 变为可读状态。
  - [EventLoop]: epoll_wait 返回，activeChannels 数组被填充。
  - [EventLoop]: 遍历 activeChannels (此时包含 clientChannel)。
      - 调用 clientChannel->handleEvent()。
      - [Channel]: 当前为 EPOLLIN 状态，进入 handleRead 分支，调用 readCallback (即 Connection::handleRead)。
          - [Connection] handleRead 执行 (业务逻辑触发点):
              - 调用 inputBuffer_.readFd(clientFd)，将数据从内核读取到 inputBuffer_。
              - 提取消息内容: msg = inputBuffer_.retrieveAllAsString()。
              - 执行回显逻辑: 调用 send(msg) -> 进入第 4 步。

4. 回送数据 (Send Response)
  - [Connection] send 执行:
      - 判断 outputBuffer_ 是否为空 且 socket 是否可写:
          - 尝试直接写入: write(clientFd, msg)。
          - 分支 A (假设一次写完): 直接返回。
          - 分支 B (假设没写完): 将剩余数据 append 到 outputBuffer_。
              - 开启监听写事件: channel_->enableWriting()。
              - [Epoll] 执行 epoll_ctl(EPOLL_CTL_MOD, clientFd, 增加 EPOLLOUT)。
  - [Connection]: send 返回 -> handleRead 返回。
  - [Channel]: handleEvent 返回。
  - [EventLoop]: 继续下一轮 loop() 循环，阻塞等待事件。
  ----- 以下为非一次性写完时的异步后续流程 -----
  - [Kernel] (稍后): socket 发送缓冲区出现空位，clientFd 变为可写状态。
  - [EventLoop]: epoll_wait 返回，触发 EPOLLOUT 事件。
  - [Channel]: 调用 clientChannel->handleEvent()，随后调用 writeCallback (即 Connection::handleWrite)。
      - [Connection] handleWrite 执行:
          - 调用 write(outputBuffer_) 发送剩余数据。
          - 分支 A (若全部发完):
              - 取消监听写事件: channel_->disableWriting() -> epoll_ctl(EPOLL_CTL_MOD, 去除 EPOLLOUT)。
              - handleWrite 返回。
          - 分支 B (若仍没发完):
              - 更新 outputBuffer_ 的剩余数据状态，等待下次事件。
              - handleWrite 返回。
  - [Channel]: handleEvent 返回。
  - [EventLoop]: 继续下一轮 loop() 循环。

5. 客户端断开 (Disconnection)
  - [Kernel]: 收到客户端的 FIN 包，clientFd 变为可读状态 (但 payload 为 0)。
  - [EventLoop]: epoll_wait 返回，activeChannels 数组被填充。
  - [EventLoop]: 遍历 activeChannels (包含 clientChannel)。
      - 调用 clientChannel->handleEvent()。
      - [Channel]: 当前为 EPOLLIN 状态，进入 handleRead 分支，调用 readCallback (即 Connection::handleRead)。
          - [Connection] handleRead 执行:
              - 调用 readFd 读取，返回值为 0 (EOF)。
              - 进入终止连接分支:
                  - 标记状态: state_ = Closed。
                  - 调用 deleteConnectionCallback (即 Server::deleteConnection)。
                      - [Server] deleteConnection 执行:
                          - 从维护容器中移除: connections.erase(clientFd)。
                          - 销毁连接: delete Connection。
                          - [Connection] 析构过程:
                              - 销毁通道: delete Channel。
                              - 销毁套接字: delete Socket -> 底层调用 close(clientFd)。
          - [Connection]: handleRead 返回。
      - [Channel]: handleEvent 返回。
  - [EventLoop]: 继续下一轮 loop() 循环。

6. 服务器关机 (Shutdown)
  - [Main]: 满足退出条件 (quit = true)，跳出 loop->loop() 的死循环。
    (注: 目前暂无信号处理逻辑，通常依靠 Ctrl+C 强杀，由 OS 回收资源)。
  - [Main] 执行资源清理:
      - 执行 delete Server -> 触发 delete Acceptor。
      - 执行 delete EventLoop -> 触发 delete Epoll -> 底层调用 close(epollFd)。
当前全流程调用链梳理 (Day 11 版本: 单 Reactor + 任务与计算线程池)
核心改变: 引入 ThreadPool。Main Loop 负责所有 IO (accept/read/write)，复杂业务逻辑封装成 Task 扔给 ThreadPool 执行。

1. 收到客户端请求 (Data Arrives)
  - [EventLoop (Main)]: 监听到 clientFd 可读。
  - [Connection]: 调用 handleRead。
      - 执行 readFd 将数据从 Kernel 读入 inputBuffer_。(IO 操作，仍在主线程)
      - [Connection]: 创建任务 Task (lambda)。
          - 注意: 此时数据已经读到了内存中。
          - 任务内容: 从 inputBuffer_ 取数据 -> 业务处理(Echo) -> 调用 send()。
          - 执行 threadPool->add(Task)。(非阻塞，瞬间完成)
  - [EventLoop (Main)]: handleRead 返回，继续处理下一个 Channel 的事件。

2. 业务处理 (Processing in Worker Thread)
  - [ThreadPool]: 某个空闲的 Worker 线程抢到了任务。
  - [Worker Thread]: 执行任务 lambda。
      - 业务计算 (如打印日志、解析协议)。
      - 调用 Connection::send(result)。(跨线程调用!)
          - 这是一个潜在的竞争点 (Race Condition)，因为主线程可能也在操作这个 Connection。
          - 但在 Day 11 的简单 Echo 场景下由于逻辑简单，暂时没暴露问题。
  - [Worker Thread]: 任务结束，线程回到空闲状态。

----------------------------------------------------------------

当前全流程调用链梳理 (Day 12 版本: 主从 Reactor 多线程)
核心改变: Reactor 分裂。Main Reactor 只负责 Accept, 将新连接分发给 Sub Reactor。每个 Sub Reactor 独占一个线程，负责其名下所有连接的 IO 和业务。

1. 启动服务器 (Startup)
  - [Main]: 创建 Main Reactor (EventLoop)。
  - [Server]: 构造函数执行。
      - new Acceptor(mainLoop)。
      - 创建 ThreadPool (假设 4 个线程)。
      - 创建 4 个 Sub Reactor (EventLoop)。
      - 循环 4 次: threadPool->add(subLoop->loop())。
          - [Worker Threads 1-4]: 开始运行 loop()，陷入 epoll_wait 死循环。
          - 此时，这 4 个线程升级为 "IO 线程"。

2. 收到连接请求 (New Connection)
  - [Main Reactor]: 收到 listenFd 可读事件。
  - [Acceptor]: accept 得到 clientFd。
  - [Server]: newConnection(clientFd) 被回调。
      - 负载均衡: int dispatchId = clientFd % 4。
      - 选取指派: subReactor = subReactors[dispatchId]。
      - **关键移交**: new Connection(subReactor, clientFd)。
          - Connection 初始化时，会创建 Channel。
          - Channel 初始化时，会调用 loop->updateChannel()。
          - 此时的 loop 是 subReactor。
          - [Sub Reactor 线程]: 将 clientFd 加入到自己的 epoll 实例中。
  - [Main Reactor]: 任务完成，继续监听下一个连接。

3. 收到客户端请求 (Data Arrives)
  - [Sub Reactor 线程]: 自己的 epoll_wait 返回 (仅负责自己分到的约 1/4 连接)。
  - [Channel]: handleEvent -> Connection::handleRead。
  - [Connection]:
      - readFd (在子线程执行)。
      - 业务逻辑 (在子线程执行)。
      - send (在子线程执行)。
  - **全程无跨线程交互**: 不需要加锁，效率极高。Connection 就像是"由于生在哪个线程，就一辈子死在哪个线程"。

4. 客户端断开 (Disconnection)
  - [Sub Reactor 线程]: readFd 返回 0。
  - [Connection]: 调用 handleClose。
  - [Server]: deleteConnection(sock) 被回调。
      - **严重隐患**: deleteConnection 在子线程被调用，但它试图去 erase 主线程 Server 持有的 connections map。
      - (这是一个 Race Condition，将在 Day 13 修复)。

---

## Day 13：工程化与线程安全

### 核心目标

Day 12 实现了主从 Reactor 多线程，但留下了三个问题：
1. `deleteConnection` 在子线程调用，却操作主线程的 `connections_` map → Race Condition。
2. 代码风格散乱，缺少统一的复制/移动禁用机制。
3. CMakeLists.txt 结构原始，不支持库复用。

### 改进一：Macros.h

引入 `Macros.h`，统一禁止所有资源持有类被复制或移动：

```cpp
#define DISALLOW_COPY(cname) \
    cname(const cname &) = delete; \
    cname &operator=(const cname &) = delete;
#define DISALLOW_MOVE(cname) \
    cname(cname &&) = delete; \
    cname &operator=(cname &&) = delete;
#define DISALLOW_COPY_AND_MOVE(cname) DISALLOW_COPY(cname); DISALLOW_MOVE(cname);
```

为什么需要这个？因为 `Epoll`、`Connection`、`EventLoop` 等类都持有 fd（文件描述符）资源。如果允许复制，析构时会 `close` 同一个 fd 两次，引发未定义行为。

### 改进二：eventfd + queueInLoop（跨线程任务投递）

这是 Day 13 最核心的改动。问题的本质：

- `connections_` map 属于**主线程（Main Reactor）**
- 断开连接事件由**子线程（Sub Reactor）**的 `epoll_wait` 检测到
- 子线程直接 `erase` 主线程的 map → 数据竞争

解决方案：不让子线程直接操作 map，而是让它向主线程投递一个任务，让主线程自己去删。

机制：`EventLoop` 增加一个任务队列 `pendingFunctors_`，任何线程都可以向里面 `push_back`，主线程的 `loop()` 在每轮事件处理完毕后，统一执行队列里的所有任务。

问题：如果 `pendingFunctors_` 里刚放了任务，但主线程正阻塞在 `epoll_wait`（没有 IO 事件）怎么办？

解决：`eventfd`。这是一个专门用于线程间通知的 fd。`EventLoop` 在初始化时创建一个 `eventfd`，并把它注册到自己的 epoll 里。每当有任务被投进来，就向这个 `eventfd` 写一个值（`write`），这会立刻让 `epoll_wait` 返回，主线程随即执行 `doPendingFunctors()`。

```
子线程检测到断开
    → deleteConnection(sock)
        → mainReactor_->queueInLoop(task)   // 把删除任务投进主线程队列
            → 向 eventfd 写 1               // 唤醒主线程的 epoll_wait
                → 主线程 loop() 执行 doPendingFunctors()
                    → 从 connections_ map 中安全删除
```

### 改进三：CMake 静态库

将所有 `common/*.cpp` 编译为静态库 `NetLib`，各可执行文件链接 `NetLib`，避免重复编译公共代码。

---

当前全流程调用链梳理 (Day 13 版本: 线程安全的连接关闭)

4. 客户端断开 (Disconnection)
  - [Sub Reactor 线程]: epoll_wait 返回，clientFd 可读但 readFd 返回 0 (EOF)。
  - [Connection::doRead()]: 检测到 n == 0，将 state_ 设为 kClosed，调用 close()。
  - [Connection::close()]: 调用 deleteConnectionCallback_(sock_)，即 Server::deleteConnection。
  - [Server::deleteConnection (在子线程执行)]:
      - **不再直接操作 map**，而是构造一个 lambda 任务 task = { erase + delete Connection }。
      - 调用 mainReactor_->queueInLoop(task)。
          - 将 task push 进 pendingFunctors_ (加锁保护)。
          - 向 evtfd_ (eventfd) 写 1，触发主线程的 epoll_wait 返回。
  - [Main Reactor 线程]: epoll_wait 被 eventfd 唤醒，在 evtChannel_ 的读回调 handleWakeup() 中读走该值 (清零)。
  - [Main Reactor 线程]: loop() 的末尾执行 doPendingFunctors():
      - 从 pendingFunctors_ swap 出任务列表 (短暂加锁)。
      - 执行 task：erase connections_[sockfd]，delete conn。
      - **全程在主线程执行，无竞争**。

---

## Day 14：状态机 + 业务框架完全解耦

### 核心目标

Day 13 之前，`Connection` 的读回调硬编码了 Echo 业务逻辑（读到数据就原样回发）。这意味着要换业务，就必须修改 `Connection.cpp`，框架和业务混在一起，完全无法复用。

Day 14 的目标：让用户（在这个项目中就是指的 server.cpp 这个代码文件）在 `main()` 里注入一个 lambda，服务器框架永远不需要知道业务是什么。

### 改进一：Connection 状态机

`Connection` 引入 `State` 枚举，明确描述连接的当前状态：

```
kInvalid → (构造完成) → kConnected → (读返回0) → kClosed
                                   → (读返回<0) → kFailed
```

业务回调里可以通过 `conn->getState()` 判断状态，决定要不要处理数据，还是要关闭连接：

```cpp
// server.cpp（业务层，不属于框架）
server->onConnect([](Connection *conn) {
    if (conn->getState() != Connection::State::kConnected) {
        conn->close();
        return;
    }
    // 处理数据...
});
```

### 改进二：onConnect() 注入业务

`Server` 新增 `onConnect(fn)` 方法，将用户提供的 lambda 存储为 `onConnectCallback_`。当有新连接时，这个 callback 被传给 `Connection::setOnConnectCallback`，Connection 的读回调直接调用它。

这样，`main()` 里的一个 lambda 就决定了服务器的全部行为，框架代码完全不感知业务。

### 调用链变化

```
Channel::setReadCallback ← Connection::handleRead (硬编码 Echo)   [Day 13 及之前]
                         ↓
Channel::setReadCallback ← Connection 的 handleRead 调用 onConnectCallback_  [Day 14]
                                                        ↑
                                          server.cpp 里注入的 lambda
```

---

当前全流程调用链梳理 (Day 14 版本: 业务逻辑完全解耦)

1. 服务器启动 (Startup)
  - [Main]: new Eventloop() → new Server(loop)。
  - [server.cpp]: 调用 server->onConnect(lambda)，将业务 lambda 存入 Server::onConnectCallback_。
  - [Main]: loop->loop()，阻塞在 epoll_wait。

2. 新连接到来 (New Connection)
  - [Acceptor]: accept 得到 clientFd。
  - [Server::newConnection()]:
      - new Connection(subReactor, clientFd)。
      - conn->setOnConnectCallback(onConnectCallback_)：
          - Connection 内部将 channel_ 的读回调绑定为 [this]{ onConnectCallback_(this); }。
      - state_ 在 Connection 构造函数末尾被设为 kConnected。

3. 数据到来 (Data Arrives)
  - [Sub Reactor]: epoll_wait 返回，在 clientFd 上检测到 EPOLLIN。
  - [Channel::handleEvent()]: 调用 readCallback。
  - [Connection::handleRead()]:
      - 调用 inputBuffer_.readFd(fd) 读入数据，n > 0。
      - 调用 onConnectCallback_(this)，即用户注入的 lambda。
          - lambda 检查 conn->getState() == kConnected → true。
          - 调用 conn->readBuffer()->retrieveAllAsString() 取出数据。
          - 调用 conn->send(msg) 回发。

4. 客户端断开 (Disconnection)
  - [Connection::handleRead()]: readFd 返回 0，state_ = kClosed，调用 onConnectCallback_(this)。
  - [用户 lambda]: conn->getState() != kConnected → 调用 conn->close()。
  - [Connection::close()]: 调用 deleteConnectionCallback_(sock_)。
  - [Server::deleteConnection()]: queueInLoop 投递删除任务到主线程（同 Day 13）。

---

## Day 15：进一步完善业务解耦 + 工具类补充

### 核心目标

Day 14 只有一个回调 `onConnectCallback_`，它要同时处理三种情形：数据到来、连接关闭、连接失败。这三种情形的处理逻辑性质不同，混在一个 lambda 里会导致 `if/else if/else` 链的迷宫。

Day 15 将回调职责**精确拆分**为两个：

| 回调 | 触发时机 | 当前状态 | 典型用途 |
|---|---|---|---|
| `newConnectCallback_` | 新连接建立，一次性触发 | `kConnected` | 打印日志、初始化会话 |
| `onMessageCallback_` | 每次有数据可读时触发 | `kConnected` | 处理消息、回发数据 |

同时，Connection 内部的 IO 读写逻辑被彻底从回调中剥离：

### 改进一：doRead / doWrite 与 Business()

原来：`handleRead()` = 读 IO + 调业务回调（混在一起）

现在：
- `doRead()` = **纯 IO**，只负责将数据读入 `inputBuffer_`，遇到断开则调 `close()`，不调任何业务回调。
- `doWrite()` = **纯 IO**，只负责将 `outputBuffer_` 的数据发出去。
- `Business()` = `Read()` + `onMessageCallback_(this)`，是 Channel 的 read callback。

`setOnMessageCallback(fn)` 被调用后，Channel 的读回调从 `doRead` 换成 `Business`：

```cpp
void Connection::setOnMessageCallback(fn) {
    onMessageCallback_ = fn;
    channel_->setReadCallback(std::bind(&Connection::Business, this));
    //      ↑ 注册后，每次 epoll 通知读事件，触发链路变为：
    //        Channel → Business() → doRead() + onMessageCallback_(this)
}
```

这样框架和业务的边界完全清晰：框架只调 `Business()`，业务只看 `onMessageCallback_`。

### 改进二：三个工具头文件

**`Exception.h`**：定义 `Exception` 类继承自 `std::runtime_error`，携带 `ExceptionType` 枚举。替代原来的 `ErrorIf(cond, msg)` 宏，在条件不满足时抛出异常，调用栈清晰，可被上层捕获。

**`SignalHandler.h`**：用 `inline` 全局 map + 函数封装 `::signal()`，让信号处理变成：
```cpp
Signal::signal(SIGINT, [&]{ loop->setQuit(); });
```
消除了 `server.cpp` 里以前那个全局 `g_loop` 变量和 `signalHandler` 函数，代码更内聚。

**`pine.h`**：一行 include 入口，引入所有框架头文件，简化使用方的 include 列表。

---

当前全流程调用链梳理 (Day 15 版本: 双回调 + 纯 IO 分离)

1. 服务器启动 (Startup)
  - [Main]: new Eventloop() → new Server(loop)。
  - [server.cpp]: 注册两个回调：
      - server->newConnect(lambda_A)：新连接通知（一次性）。
      - server->onMessage(lambda_B)：消息处理（每次数据到来）。
  - [Main]: loop->loop()，阻塞在 epoll_wait。

2. 新连接到来 (New Connection)
  - [Acceptor]: accept → clientFd。
  - [Server::newConnection()]:
      - new Connection(subReactor, clientFd)：构造函数内：
          - channel_ 的读回调初始被绑定为 Connection::doRead（仅做 IO，尚无业务）。
          - state_ = kConnected。
      - conn->setOnMessageCallback(onMessageCallback_)：
          - 将 lambda_B 存入 onMessageCallback_。
          - **重绑 channel 读回调** 为 Connection::Business。
      - if (newConnectCallback_) newConnectCallback_(conn)：
          - 调用 lambda_A，打印"新连接"日志等。
          - 注意：此时 channel 读回调已经是 Business，newConnectCallback_ 只是一次性通知，不参与数据流。

3. 数据到来 (Data Arrives)
  - [Sub Reactor]: epoll_wait 返回，EPOLLIN 触发。
  - [Channel::handleEvent()]: 调用 readCallback，即 Connection::Business()。
  - [Connection::Business()]:
      - 调用 Read()，即 doRead()：
          - inputBuffer_.readFd(fd)：将数据读入 inputBuffer_。
          - n > 0：什么都不做，返回给 Business()。
          - n == 0：state_ = kClosed，调用 close()，触发断开流程，Business() 后续的 onMessageCallback_ 不会被调（doRead 里直接 return 了，但实际上 Business 还会调 onMessageCallback_，业务需自行判断 state）。
      - 调用 onMessageCallback_(this)，即 lambda_B：
          - 业务检查 conn->getState()。
          - 取数据、回发。

4. 客户端断开 (Disconnection)
  - [Connection::doRead()]: n == 0，state_ = kClosed，调用 close()。
  - [Server::deleteConnection()]: queueInLoop 投递到主线程（同 Day 13/14）。
  - 注意：doRead 调用 close() 后随即返回，随后 Business() 仍会调 onMessageCallback_。
    这意味着 **业务 lambda 有责任检查 `conn->getState() == kConnected` 后再操作**，否则会对一个已关闭的连接做无意义的读写。

5. 捕获 SIGINT 关机 (Shutdown)
  - [Signal::signal()]: SIGINT 触发 `handlers_[SIGINT]()`，即 lambda { loop->setQuit() }。
  - [Eventloop::loop()]: quit_ == true，退出 while 循环。
  - [Main]: delete server → delete loop，析构链清理所有资源。
